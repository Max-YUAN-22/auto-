#!/usr/bin/env python3
"""
å¯æ‰©å±•æ€§å®éªŒæ¡†æ¶ - å®Œå…¨çœŸå®çš„APIè°ƒç”¨å®éªŒ
Scalability Study Framework - Fully Real API Call Experiments

æµ‹è¯•ä¸åŒæ¡†æ¶åœ¨ä¸åŒè§„æ¨¡ä¸‹çš„æ€§èƒ½è¡¨ç°ï¼š
1. ä»£ç†æ•°é‡æ‰©å±•æ€§ (1-20ä¸ªä»£ç†)
2. ä»»åŠ¡å¤æ‚åº¦æ‰©å±•æ€§
3. å¹¶å‘ä»»åŠ¡æ‰©å±•æ€§
4. å†…å­˜ä½¿ç”¨æ‰©å±•æ€§
5. å»¶è¿Ÿæ‰©å±•æ€§
"""

import os
import sys
import json
import time
import logging
import numpy as np
import psutil
import gc
from datetime import datetime
from typing import Dict, List, Any, Tuple
from contextlib import contextmanager
import concurrent.futures
import threading

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = os.path.join(os.path.dirname(__file__), '..')
sys.path.insert(0, project_root)

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class ScalabilityStudy:
    """å¯æ‰©å±•æ€§å®éªŒç±»"""
    
    def __init__(self, random_seed: int = 42):
        self.random_seed = random_seed
        np.random.seed(random_seed)
        
        # åŠ è½½ç¯å¢ƒå˜é‡
        self.load_env()
        
        # è®¾ç½®APIé…ç½®
        self.api_key = os.getenv("OPENAI_API_KEY", "sk-0WQRDm5w3t3ukRFQ7j33rOUgLk9ezcTuwhsK7BXxgfyhYuqA").strip()
        self.base_url = "https://www.yunqiaoai.top/v1"
        
        # å†…å­˜è·Ÿè¸ªå™¨
        self.memory_tracker = {}
        
        # å¯æ‰©å±•æ€§å®éªŒé…ç½®
        self.scalability_config = {
            "frameworks": ["Our DSL", "LangChain", "CrewAI", "AutoGen"],
            "agent_scales": [1, 2, 3, 4, 5, 6, 8, 10, 12, 15, 18, 20],  # æ›´å¤šä»£ç†æ•°é‡
            "task_scales": [1, 2, 3, 5, 8, 10, 15, 20],  # ä»»åŠ¡æ•°é‡
            "complexity_levels": ["simple", "medium", "complex"],
            "concurrent_levels": [1, 2, 3, 4, 5],  # å¹¶å‘çº§åˆ«
            "repetitions": 3
        }
        
        # æµ‹è¯•ä»»åŠ¡
        self.tasks = self._generate_scalability_tasks()
        
        logger.info(f"å¯æ‰©å±•æ€§å®éªŒåˆå§‹åŒ–å®Œæˆ - æ¡†æ¶æ•°: {len(self.scalability_config['frameworks'])}, "
                   f"ä»£ç†è§„æ¨¡: {len(self.scalability_config['agent_scales'])}, "
                   f"ä»»åŠ¡è§„æ¨¡: {len(self.scalability_config['task_scales'])}")
    
    def load_env(self):
        """åŠ è½½ç¯å¢ƒå˜é‡"""
        os.environ['OPENAI_API_KEY'] = 'sk-0WQRDm5w3t3ukRFQ7j33rOUgLk9ezcTuwhsK7BXxgfyhYuqA'
        logger.info("è®¾ç½®OPENAI_API_KEYç¯å¢ƒå˜é‡")
    
    def _generate_scalability_tasks(self) -> Dict[str, List[str]]:
        """ç”Ÿæˆå¯æ‰©å±•æ€§æµ‹è¯•ä»»åŠ¡"""
        tasks = {
            "simple": [
                "åˆ†æä¸€å®¶å°å‹å’–å•¡åº—çš„æœˆåº¦é”€å”®æ•°æ®ï¼Œè¯†åˆ«é”€å”®è¶‹åŠ¿å¹¶æä¾›æ”¹è¿›å»ºè®®ã€‚",
                "è¯„ä¼°ä¸€ä¸ªåœ¨çº¿æ•™è‚²å¹³å°çš„ç”¨æˆ·å¢é•¿ç­–ç•¥ï¼Œæå‡ºä¼˜åŒ–æ–¹æ¡ˆã€‚",
                "è®¾è®¡ä¸€ä¸ªç®€å•çš„RESTful APIæ¶æ„ï¼ŒåŒ…æ‹¬è®¤è¯ã€æˆæƒå’Œæ•°æ®éªŒè¯æœºåˆ¶ã€‚",
                "ä¸ºç§»åŠ¨åº”ç”¨è®¾è®¡ç”¨æˆ·è®¤è¯å’Œä¼šè¯ç®¡ç†ç³»ç»Ÿã€‚",
                "åˆ†ææ°”å€™å˜åŒ–å¯¹å†œä¸šäº§é‡çš„å½±å“ï¼ŒåŸºäºå†å²æ•°æ®å»ºç«‹é¢„æµ‹æ¨¡å‹ã€‚",
                "ç ”ç©¶ç¤¾äº¤åª’ä½“å¯¹é’å°‘å¹´å¿ƒç†å¥åº·çš„å½±å“ï¼Œè®¾è®¡å®éªŒæ–¹æ¡ˆã€‚",
                "ä¼˜åŒ–ä¸€ä¸ªWebåº”ç”¨çš„æ•°æ®åº“æŸ¥è¯¢æ€§èƒ½ï¼Œå‡å°‘å“åº”æ—¶é—´ã€‚",
                "ä¼˜åŒ–ä¸€ä¸ªç§»åŠ¨åº”ç”¨çš„ç”µæ± ä½¿ç”¨æ•ˆç‡ã€‚",
                "è¯„ä¼°ä¸€ä¸ªåˆ›ä¸šé¡¹ç›®çš„å¸‚åœºé£é™©å’Œè´¢åŠ¡é£é™©ã€‚",
                "è¯„ä¼°ä¸€ä¸ªITé¡¹ç›®çš„æŠ€æœ¯é£é™©å’Œè¿›åº¦é£é™©ã€‚",
                "åˆ¶å®šä¸€ä¸ªåˆåˆ›å…¬å¸çš„3å¹´å‘å±•æˆ˜ç•¥è§„åˆ’ã€‚",
                "åˆ¶å®šä¸€ä¸ªä¼ ç»Ÿä¼ä¸šçš„æ•°å­—åŒ–è½¬å‹æˆ˜ç•¥ã€‚",
                "åˆ†æç”µå•†å¹³å°çš„ç”¨æˆ·è¡Œä¸ºæ•°æ®ï¼Œè¯†åˆ«è´­ä¹°æ¨¡å¼å’Œç”¨æˆ·åå¥½ã€‚",
                "åˆ†æè‚¡ç¥¨å¸‚åœºçš„ä»·æ ¼æ³¢åŠ¨ï¼Œå»ºç«‹ç®€å•çš„é¢„æµ‹æ¨¡å‹ã€‚",
                "åˆ›ä½œä¸€ä¸ªå…³äºäººå·¥æ™ºèƒ½ä¸äººç±»åˆä½œçš„çŸ­ç¯‡ç§‘å¹»æ•…äº‹ã€‚",
                "ç¼–å†™ä¸€ä¸ªå…³äºç¯ä¿ä¸»é¢˜çš„å„¿ç«¥æ•™è‚²å‰§æœ¬ã€‚",
                "è®¾è®¡ä¸€ä¸ªç®€å•çš„RESTful APIæ¶æ„ï¼ŒåŒ…æ‹¬è®¤è¯ã€æˆæƒå’Œæ•°æ®éªŒè¯æœºåˆ¶ã€‚",
                "ä¸ºç§»åŠ¨åº”ç”¨è®¾è®¡ç”¨æˆ·è®¤è¯å’Œä¼šè¯ç®¡ç†ç³»ç»Ÿã€‚",
                "åˆ†ææ°”å€™å˜åŒ–å¯¹å†œä¸šäº§é‡çš„å½±å“ï¼ŒåŸºäºå†å²æ•°æ®å»ºç«‹é¢„æµ‹æ¨¡å‹ã€‚",
                "ç ”ç©¶ç¤¾äº¤åª’ä½“å¯¹é’å°‘å¹´å¿ƒç†å¥åº·çš„å½±å“ï¼Œè®¾è®¡å®éªŒæ–¹æ¡ˆã€‚"
            ],
            "medium": [
                "ä¸ºä¸€å®¶ä¸­å‹åˆ¶é€ ä¼ä¸šè®¾è®¡æ•°å­—åŒ–è½¬å‹è·¯çº¿å›¾ï¼ŒåŒ…æ‹¬æŠ€æœ¯é€‰å‹ã€å®æ–½è®¡åˆ’å’Œé£é™©è¯„ä¼°ã€‚",
                "åˆ†æç”µå•†å¹³å°çš„ä¾›åº”é“¾ç®¡ç†é—®é¢˜ï¼Œè®¾è®¡ä¼˜åŒ–æ–¹æ¡ˆä»¥æé«˜æ•ˆç‡å’Œé™ä½æˆæœ¬ã€‚",
                "è®¾è®¡ä¸€ä¸ªå¾®æœåŠ¡æ¶æ„çš„ç”µå•†å¹³å°ï¼ŒåŒ…æ‹¬ç”¨æˆ·æœåŠ¡ã€å•†å“æœåŠ¡ã€è®¢å•æœåŠ¡å’Œæ”¯ä»˜æœåŠ¡ï¼Œè€ƒè™‘æœåŠ¡é—´é€šä¿¡ã€æ•°æ®ä¸€è‡´æ€§å’Œå®¹é”™æœºåˆ¶ã€‚",
                "è®¾è®¡ä¸€ä¸ªåˆ†å¸ƒå¼ç¼“å­˜ç³»ç»Ÿï¼Œæ”¯æŒé«˜å¹¶å‘è¯»å†™ã€æ•°æ®ä¸€è‡´æ€§å’Œæ•…éšœæ¢å¤ã€‚",
                "è®¾è®¡ä¸€ä¸ªåŸºäºæœºå™¨å­¦ä¹ çš„è¯ç‰©å‘ç°å¹³å°ï¼ŒåŒ…æ‹¬åˆ†å­ç­›é€‰ã€æ´»æ€§é¢„æµ‹å’Œæ¯’æ€§è¯„ä¼°ï¼Œè€ƒè™‘æ•°æ®è´¨é‡å’Œæ¨¡å‹å¯è§£é‡Šæ€§ã€‚",
                "ç ”ç©¶é‡å­è®¡ç®—åœ¨å¯†ç å­¦ä¸­çš„åº”ç”¨ï¼Œåˆ†æé‡å­ç®—æ³•å¯¹ç°æœ‰åŠ å¯†ç³»ç»Ÿçš„å½±å“å¹¶æå‡ºåé‡å­å¯†ç å­¦æ–¹æ¡ˆã€‚",
                "ä¼˜åŒ–ä¸€ä¸ªåˆ†å¸ƒå¼ç³»ç»Ÿçš„èµ„æºåˆ©ç”¨ç‡ï¼ŒåŒ…æ‹¬CPUã€å†…å­˜ã€ç½‘ç»œå’Œå­˜å‚¨ï¼Œå®ç°åŠ¨æ€è´Ÿè½½å‡è¡¡å’Œèµ„æºè°ƒåº¦ã€‚",
                "ä¼˜åŒ–ä¸€ä¸ªæœºå™¨å­¦ä¹ è®­ç»ƒç®¡é“çš„æ•ˆç‡ï¼ŒåŒ…æ‹¬æ•°æ®é¢„å¤„ç†ã€æ¨¡å‹è®­ç»ƒå’Œè¶…å‚æ•°è°ƒä¼˜ã€‚",
                "è¯„ä¼°ä¸€ä¸ªè·¨å›½ä¼ä¸šçš„è¿è¥é£é™©ï¼ŒåŒ…æ‹¬æ”¿æ²»é£é™©ã€æ±‡ç‡é£é™©ã€ä¾›åº”é“¾é£é™©å’Œåˆè§„é£é™©ã€‚",
                "è¯„ä¼°ä¸€ä¸ªé‡‘èç§‘æŠ€å…¬å¸çš„ç³»ç»Ÿæ€§é£é™©ï¼ŒåŒ…æ‹¬æŠ€æœ¯é£é™©ã€å¸‚åœºé£é™©ã€æ“ä½œé£é™©å’Œç›‘ç®¡é£é™©ã€‚",
                "åˆ¶å®šä¸€ä¸ªä¸­å‹ä¼ä¸šçš„å›½é™…åŒ–æ‰©å¼ æˆ˜ç•¥ï¼ŒåŒ…æ‹¬å¸‚åœºé€‰æ‹©ã€è¿›å…¥æ¨¡å¼ã€èµ„æºé…ç½®å’Œé£é™©ç®¡æ§ã€‚",
                "åˆ¶å®šä¸€ä¸ªç§‘æŠ€å…¬å¸çš„äº§å“åˆ›æ–°æˆ˜ç•¥ï¼ŒåŒ…æ‹¬æŠ€æœ¯è·¯çº¿å›¾ã€å¸‚åœºå®šä½ã€ç«äº‰ç­–ç•¥å’Œå•†ä¸šæ¨¡å¼ã€‚",
                "åˆ†æåŸå¸‚äº¤é€šæ•°æ®ï¼Œè®¾è®¡æ™ºèƒ½äº¤é€šç®¡ç†ç³»ç»Ÿä»¥å‡å°‘æ‹¥å µå’Œæé«˜æ•ˆç‡ã€‚",
                "åˆ†æåŒ»ç–—æ•°æ®ï¼Œå»ºç«‹ç–¾ç—…é¢„æµ‹æ¨¡å‹å¹¶è¯„ä¼°å…¶ä¸´åºŠä»·å€¼ã€‚",
                "åˆ›ä½œä¸€ä¸ªå¤šçº¿ç¨‹çš„æ‚¬ç–‘å°è¯´å¤§çº²ï¼ŒåŒ…å«å¤šä¸ªè§†è§’å’Œå¤æ‚çš„äººç‰©å…³ç³»ã€‚",
                "ç¼–å†™ä¸€ä¸ªå…³äºæœªæ¥åŸå¸‚ç”Ÿæ´»çš„äº’åŠ¨å¼æ•°å­—å™äº‹ä½œå“ã€‚",
                "è®¾è®¡ä¸€ä¸ªå¾®æœåŠ¡æ¶æ„çš„ç”µå•†å¹³å°ï¼ŒåŒ…æ‹¬ç”¨æˆ·æœåŠ¡ã€å•†å“æœåŠ¡ã€è®¢å•æœåŠ¡å’Œæ”¯ä»˜æœåŠ¡ï¼Œè€ƒè™‘æœåŠ¡é—´é€šä¿¡ã€æ•°æ®ä¸€è‡´æ€§å’Œå®¹é”™æœºåˆ¶ã€‚",
                "è®¾è®¡ä¸€ä¸ªåˆ†å¸ƒå¼ç¼“å­˜ç³»ç»Ÿï¼Œæ”¯æŒé«˜å¹¶å‘è¯»å†™ã€æ•°æ®ä¸€è‡´æ€§å’Œæ•…éšœæ¢å¤ã€‚",
                "è®¾è®¡ä¸€ä¸ªåŸºäºæœºå™¨å­¦ä¹ çš„è¯ç‰©å‘ç°å¹³å°ï¼ŒåŒ…æ‹¬åˆ†å­ç­›é€‰ã€æ´»æ€§é¢„æµ‹å’Œæ¯’æ€§è¯„ä¼°ï¼Œè€ƒè™‘æ•°æ®è´¨é‡å’Œæ¨¡å‹å¯è§£é‡Šæ€§ã€‚",
                "ç ”ç©¶é‡å­è®¡ç®—åœ¨å¯†ç å­¦ä¸­çš„åº”ç”¨ï¼Œåˆ†æé‡å­ç®—æ³•å¯¹ç°æœ‰åŠ å¯†ç³»ç»Ÿçš„å½±å“å¹¶æå‡ºåé‡å­å¯†ç å­¦æ–¹æ¡ˆã€‚"
            ],
            "complex": [
                "è®¾è®¡ä¸€ä¸ªå¤šå›½ä¼ä¸šçš„å…¨çƒä¾›åº”é“¾é£é™©ç®¡ç†æ¡†æ¶ï¼Œè€ƒè™‘åœ°ç¼˜æ”¿æ²»ã€è‡ªç„¶ç¾å®³ã€å¸‚åœºæ³¢åŠ¨ç­‰å› ç´ ï¼Œå¹¶æä¾›å®æ—¶ç›‘æ§å’Œåº”æ€¥å“åº”æœºåˆ¶ã€‚",
                "ä¸ºä¸€å®¶å¤§å‹é‡‘èæœºæ„è®¾è®¡åŸºäºAIçš„æ™ºèƒ½æŠ•é¡¾ç³»ç»Ÿæ¶æ„ï¼ŒåŒ…æ‹¬å®¢æˆ·ç”»åƒã€é£é™©è¯„ä¼°ã€æŠ•èµ„ç»„åˆä¼˜åŒ–å’Œåˆè§„ç®¡ç†ã€‚",
                "è®¾è®¡ä¸€ä¸ªæ”¯æŒåƒä¸‡çº§ç”¨æˆ·çš„å®æ—¶æ¨èç³»ç»Ÿæ¶æ„ï¼ŒåŒ…æ‹¬æ•°æ®æ”¶é›†ã€ç‰¹å¾å·¥ç¨‹ã€æ¨¡å‹è®­ç»ƒã€åœ¨çº¿æ¨ç†å’ŒA/Bæµ‹è¯•æ¡†æ¶ï¼Œè€ƒè™‘å»¶è¿Ÿã€å‡†ç¡®æ€§å’Œå¯æ‰©å±•æ€§ã€‚",
                "è®¾è®¡ä¸€ä¸ªå¤šäº‘ç¯å¢ƒä¸‹çš„å®¹å™¨ç¼–æ’å¹³å°ï¼Œæ”¯æŒè‡ªåŠ¨æ‰©ç¼©å®¹ã€æœåŠ¡å‘ç°ã€è´Ÿè½½å‡è¡¡ã€ç›‘æ§å‘Šè­¦å’Œç¾éš¾æ¢å¤ã€‚",
                "è®¾è®¡ä¸€ä¸ªå¤šæ¨¡æ€AIç³»ç»Ÿç”¨äºç™Œç—‡æ—©æœŸè¯Šæ–­ï¼Œæ•´åˆå½±åƒå­¦ã€åŸºå› ç»„å­¦ã€è›‹ç™½è´¨ç»„å­¦å’Œä¸´åºŠæ•°æ®ï¼Œå»ºç«‹ç«¯åˆ°ç«¯çš„è¯Šæ–­å’Œé¢„åé¢„æµ‹æ¨¡å‹ã€‚",
                "ç ”ç©¶å¯æ§æ ¸èšå˜ååº”å †çš„ç­‰ç¦»å­ä½“æ§åˆ¶ç®—æ³•ï¼Œè®¾è®¡å®æ—¶æ§åˆ¶ç³»ç»Ÿä»¥ç»´æŒç­‰ç¦»å­ä½“ç¨³å®šæ€§å’Œä¼˜åŒ–èƒ½é‡è¾“å‡ºã€‚",
                "ä¼˜åŒ–ä¸€ä¸ªè¶…å¤§è§„æ¨¡äº‘è®¡ç®—å¹³å°çš„æ•´ä½“æ€§èƒ½ï¼ŒåŒ…æ‹¬è®¡ç®—ã€å­˜å‚¨ã€ç½‘ç»œã€å®‰å…¨å’Œæˆæœ¬ä¼˜åŒ–ï¼Œå®ç°å¤šç§Ÿæˆ·éš”ç¦»å’Œå¼¹æ€§æ‰©ç¼©å®¹ã€‚",
                "ä¼˜åŒ–ä¸€ä¸ªè‡ªåŠ¨é©¾é©¶ç³»ç»Ÿçš„å®æ—¶æ€§èƒ½ï¼ŒåŒ…æ‹¬æ„ŸçŸ¥ã€å†³ç­–ã€æ§åˆ¶å’Œé€šä¿¡ï¼Œç¡®ä¿å®‰å…¨æ€§å’Œæ•ˆç‡çš„å¹³è¡¡ã€‚",
                "è¯„ä¼°ä¸€ä¸ªå…¨çƒä¾›åº”é“¾ç½‘ç»œçš„ç»¼åˆé£é™©ï¼ŒåŒ…æ‹¬åœ°ç¼˜æ”¿æ²»ã€è‡ªç„¶ç¾å®³ã€ç½‘ç»œå®‰å…¨ã€æ°”å€™å˜åŒ–å’Œå®è§‚ç»æµæ³¢åŠ¨ç­‰å¤šç»´åº¦é£é™©ï¼Œå»ºç«‹åŠ¨æ€é£é™©è¯„ä¼°å’Œç¼“è§£ä½“ç³»ã€‚",
                "è¯„ä¼°ä¸€ä¸ªæ™ºèƒ½åŸå¸‚ç³»ç»Ÿçš„ç½‘ç»œå®‰å…¨é£é™©ï¼ŒåŒ…æ‹¬å…³é”®åŸºç¡€è®¾æ–½ä¿æŠ¤ã€æ•°æ®éšç§ã€AIç³»ç»Ÿå®‰å…¨å’Œåº”æ€¥å“åº”èƒ½åŠ›ã€‚",
                "åˆ¶å®šä¸€ä¸ªå¤§å‹é›†å›¢çš„å¤šå…ƒåŒ–å‘å±•æˆ˜ç•¥ï¼ŒåŒ…æ‹¬äº§ä¸šå¸ƒå±€ã€èµ„æºé…ç½®ã€ç»„ç»‡å˜é©ã€æ–‡åŒ–èåˆå’Œå¯æŒç»­å‘å±•ï¼Œå»ºç«‹åŠ¨æ€æˆ˜ç•¥è°ƒæ•´æœºåˆ¶ã€‚",
                "åˆ¶å®šä¸€ä¸ªå›½å®¶çš„æ•°å­—ç»æµå‘å±•æˆ˜ç•¥ï¼ŒåŒ…æ‹¬åŸºç¡€è®¾æ–½å»ºè®¾ã€äº§ä¸šå‡çº§ã€äººæ‰åŸ¹å…»ã€æ”¿ç­–æ³•è§„å’Œå›½é™…åˆä½œï¼Œæ„å»ºå®Œæ•´çš„æ•°å­—ç»æµç”Ÿæ€ç³»ç»Ÿã€‚",
                "åˆ†æå¤§è§„æ¨¡å¤šæºå¼‚æ„æ•°æ®ï¼ˆåŒ…æ‹¬æ–‡æœ¬ã€å›¾åƒã€ä¼ æ„Ÿå™¨æ•°æ®ï¼‰ï¼Œå»ºç«‹ç»¼åˆæ€§çš„åŸå¸‚æ™ºæ…§ç®¡ç†ç³»ç»Ÿï¼Œå®ç°å®æ—¶å†³ç­–å’Œé¢„æµ‹æ€§ç»´æŠ¤ã€‚",
                "åˆ†æé‡‘èå¸‚åœºçš„å¤šç»´åº¦æ•°æ®ï¼Œå»ºç«‹é«˜é¢‘äº¤æ˜“ç­–ç•¥å’Œé£é™©ç®¡ç†ç³»ç»Ÿï¼Œè€ƒè™‘å¸‚åœºå¾®è§‚ç»“æ„ã€æµåŠ¨æ€§é£é™©å’Œç›‘ç®¡åˆè§„ã€‚",
                "åˆ›ä½œä¸€ä¸ªè·¨åª’ä½“çš„ç§‘å¹»å²è¯—ï¼ŒåŒ…æ‹¬å°è¯´ã€æ¸¸æˆã€å½±è§†å’Œè™šæ‹Ÿç°å®ä½“éªŒï¼Œæ„å»ºå®Œæ•´çš„ä¸–ç•Œè§‚å’Œè§’è‰²ä½“ç³»ã€‚",
                "ç¼–å†™ä¸€ä¸ªåŸºäºå†å²äº‹ä»¶çš„æ²‰æµ¸å¼æˆå‰§ä½œå“ï¼Œèåˆä¼ ç»Ÿæˆå‰§ã€ç°ä»£æŠ€æœ¯å’Œè§‚ä¼—å‚ä¸å…ƒç´ ã€‚",
                "è®¾è®¡ä¸€ä¸ªæ”¯æŒåƒä¸‡çº§ç”¨æˆ·çš„å®æ—¶æ¨èç³»ç»Ÿæ¶æ„ï¼ŒåŒ…æ‹¬æ•°æ®æ”¶é›†ã€ç‰¹å¾å·¥ç¨‹ã€æ¨¡å‹è®­ç»ƒã€åœ¨çº¿æ¨ç†å’ŒA/Bæµ‹è¯•æ¡†æ¶ï¼Œè€ƒè™‘å»¶è¿Ÿã€å‡†ç¡®æ€§å’Œå¯æ‰©å±•æ€§ã€‚",
                "è®¾è®¡ä¸€ä¸ªå¤šäº‘ç¯å¢ƒä¸‹çš„å®¹å™¨ç¼–æ’å¹³å°ï¼Œæ”¯æŒè‡ªåŠ¨æ‰©ç¼©å®¹ã€æœåŠ¡å‘ç°ã€è´Ÿè½½å‡è¡¡ã€ç›‘æ§å‘Šè­¦å’Œç¾éš¾æ¢å¤ã€‚",
                "è®¾è®¡ä¸€ä¸ªå¤šæ¨¡æ€AIç³»ç»Ÿç”¨äºç™Œç—‡æ—©æœŸè¯Šæ–­ï¼Œæ•´åˆå½±åƒå­¦ã€åŸºå› ç»„å­¦ã€è›‹ç™½è´¨ç»„å­¦å’Œä¸´åºŠæ•°æ®ï¼Œå»ºç«‹ç«¯åˆ°ç«¯çš„è¯Šæ–­å’Œé¢„åé¢„æµ‹æ¨¡å‹ã€‚",
                "ç ”ç©¶å¯æ§æ ¸èšå˜ååº”å †çš„ç­‰ç¦»å­ä½“æ§åˆ¶ç®—æ³•ï¼Œè®¾è®¡å®æ—¶æ§åˆ¶ç³»ç»Ÿä»¥ç»´æŒç­‰ç¦»å­ä½“ç¨³å®šæ€§å’Œä¼˜åŒ–èƒ½é‡è¾“å‡ºã€‚"
            ]
        }
        
        return tasks
    
    @contextmanager
    def memory_tracking(self, framework: str, scale_type: str, scale_value: int, complexity: str):
        """å†…å­˜è·Ÿè¸ªä¸Šä¸‹æ–‡ç®¡ç†å™¨"""
        class MemoryTracker:
            def __init__(self, parent, framework, scale_type, scale_value, complexity):
                self.parent = parent
                self.framework = framework
                self.scale_type = scale_type
                self.scale_value = scale_value
                self.complexity = complexity
                self.initial_memory = None
                
            def __enter__(self):
                gc.collect()
                process = psutil.Process()
                self.initial_memory = process.memory_info().rss / 1024 / 1024  # MB
                return self
                
            def __exit__(self, exc_type, exc_val, exc_tb):
                process = psutil.Process()
                final_memory = process.memory_info().rss / 1024 / 1024  # MB
                memory_usage = max(0, final_memory - self.initial_memory)
                
                key = f"{self.framework}_{self.scale_type}_{self.scale_value}_{self.complexity}"
                self.parent.memory_tracker[key] = memory_usage
                logger.info(f"å†…å­˜ä½¿ç”¨è®°å½•: {key} = {memory_usage:.2f} MB")
        
        tracker = MemoryTracker(self, framework, scale_type, scale_value, complexity)
        try:
            yield tracker
        finally:
            pass
    
    def test_framework_scalability(self, framework: str, scale_type: str, scale_value: int, 
                                 complexity: str, task_count: int) -> Dict[str, Any]:
        """æµ‹è¯•æ¡†æ¶å¯æ‰©å±•æ€§"""
        try:
            tasks = self.tasks[complexity][:task_count]
            
            with self.memory_tracking(framework, scale_type, scale_value, complexity):
                start_time = time.time()
                
                if framework == "Our DSL":
                    result = self._test_our_dsl_scalability(tasks, scale_value)
                elif framework == "LangChain":
                    result = self._test_langchain_scalability(tasks, scale_value)
                elif framework == "CrewAI":
                    result = self._test_crewai_scalability(tasks, scale_value)
                elif framework == "AutoGen":
                    result = self._test_autogen_scalability(tasks, scale_value)
                
                execution_time = time.time() - start_time
                throughput = result["successful_tasks"] / execution_time if execution_time > 0 else 0
                avg_latency = execution_time / result["successful_tasks"] if result["successful_tasks"] > 0 else 0
                success_rate = (result["successful_tasks"] / len(tasks)) * 100
                
            # åœ¨withè¯­å¥ç»“æŸåè·å–å†…å­˜ä½¿ç”¨
            memory_usage = self.memory_tracker.get(f"{framework}_{scale_type}_{scale_value}_{complexity}", 0)
            
            return {
                "framework": framework,
                "scale_type": scale_type,
                "scale_value": scale_value,
                "complexity": complexity,
                "task_count": task_count,
                "execution_time": execution_time,
                "throughput": throughput,
                "success_rate": success_rate,
                "successful_tasks": result["successful_tasks"],
                "total_tasks": len(tasks),
                "avg_latency": avg_latency * 1000,  # è½¬æ¢ä¸ºms
                "status": "success",
                "memory_usage": memory_usage,
                "api_type": "real_api"
            }
                
        except Exception as e:
            logger.error(f"æ¡†æ¶å¯æ‰©å±•æ€§æµ‹è¯•å¤±è´¥ ({framework}): {e}")
            return {
                "framework": framework,
                "scale_type": scale_type,
                "scale_value": scale_value,
                "complexity": complexity,
                "task_count": task_count,
                "execution_time": 0,
                "throughput": 0,
                "success_rate": 0,
                "successful_tasks": 0,
                "total_tasks": len(self.tasks[complexity][:task_count]),
                "avg_latency": 0,
                "status": "error",
                "memory_usage": 0,
                "error": str(e),
                "api_type": "error"
            }
    
    def _test_our_dsl_scalability(self, tasks: List[str], agent_count: int) -> Dict[str, int]:
        """æµ‹è¯•Our DSLå¯æ‰©å±•æ€§"""
        from dsl.dsl import DSL
        from core.robust_llm import llm_callable
        
        # åˆ›å»ºDSLå®ä¾‹
        dsl = DSL(seed=self.random_seed, workers=min(agent_count, 8))
        
        # æ·»åŠ ä»»åŠ¡
        task_objects = []
        for i, task_prompt in enumerate(tasks):
            task_obj = dsl.gen(f"task_{i}", prompt=task_prompt, agent=f"agent_{i % agent_count}").schedule()
            task_objects.append(task_obj)
        
        # è¿è¡ŒDSL
        dsl.run(llm_callable)
        
        # ç­‰å¾…ä»»åŠ¡å®Œæˆ
        successful_tasks = 0
        for task in task_objects:
            try:
                result = task.wait(timeout=120.0)
                if result is not None and len(str(result).strip()) > 100:
                    successful_tasks += 1
            except Exception as e:
                logger.warning(f"ä»»åŠ¡ç­‰å¾…å¤±è´¥: {e}")
        
        return {"successful_tasks": successful_tasks}
    
    def _test_langchain_scalability(self, tasks: List[str], agent_count: int) -> Dict[str, int]:
        """æµ‹è¯•LangChainå¯æ‰©å±•æ€§"""
        from langchain_openai import ChatOpenAI
        from langchain.schema import HumanMessage
        
        # åˆ›å»ºLangChainå®¢æˆ·ç«¯
        llm = ChatOpenAI(
            model="gpt-4o-mini",
            api_key=self.api_key,
            base_url=self.base_url,
            temperature=0.3,
            max_tokens=1000
        )
        
        # æ‰§è¡Œä»»åŠ¡
        successful_tasks = 0
        for i, task_prompt in enumerate(tasks):
            try:
                message = HumanMessage(content=task_prompt)
                response = llm.invoke([message])
                if response and response.content and len(response.content.strip()) > 100:
                    successful_tasks += 1
            except Exception as e:
                logger.warning(f"LangChainä»»åŠ¡{i}å¤±è´¥: {e}")
        
        return {"successful_tasks": successful_tasks}
    
    def _test_crewai_scalability(self, tasks: List[str], agent_count: int) -> Dict[str, int]:
        """æµ‹è¯•CrewAIå¯æ‰©å±•æ€§"""
        from crewai import Agent, Task, Crew, Process
        from crewai.llm import LLM
        
        # åˆ›å»ºCrewAI LLM
        llm = LLM(
            model="gpt-4o-mini",
            api_key=self.api_key,
            base_url=self.base_url,
            temperature=0.3,
            max_tokens=1000
        )
        
        # åˆ›å»ºä»£ç†
        agents = []
        for i in range(min(agent_count, 3)):  # CrewAIé™åˆ¶ä»£ç†æ•°é‡
            agent = Agent(
                role=f"Specialist_{i}",
                goal=f"Complete tasks efficiently and accurately",
                backstory="You are an expert with deep domain knowledge and analytical skills.",
                llm=llm,
                verbose=False,
                allow_delegation=False
            )
            agents.append(agent)
        
        # åˆ›å»ºä»»åŠ¡
        crew_tasks = []
        for i, task_prompt in enumerate(tasks):
            task = Task(
                description=task_prompt,
                agent=agents[i % len(agents)],
                expected_output="A detailed and comprehensive response with actionable insights",
                async_execution=False
            )
            crew_tasks.append(task)
        
        # åˆ›å»ºCrewå¹¶æ‰§è¡Œ
        crew = Crew(
            agents=agents,
            tasks=crew_tasks,
            verbose=False,
            process=Process.sequential
        )
        
        result = crew.kickoff()
        
        # è®¡ç®—æˆåŠŸä»»åŠ¡æ•°
        successful_tasks = 0
        if result:
            result_str = str(result).strip()
            if len(result_str) > 100:
                # æ£€æŸ¥æ˜¯å¦åŒ…å«å¤šä¸ªä»»åŠ¡çš„å“åº”
                task_responses = result_str.split('\n\n')
                successful_tasks = min(len(task_responses), len(tasks))
            else:
                successful_tasks = 1 if len(result_str) > 100 else 0
        
        return {"successful_tasks": successful_tasks}
    
    def _test_autogen_scalability(self, tasks: List[str], agent_count: int) -> Dict[str, int]:
        """æµ‹è¯•AutoGenå¯æ‰©å±•æ€§"""
        from autogen import ConversableAgent, GroupChat, GroupChatManager
        
        # é…ç½®LLM
        llm_config = {
            "model": "gpt-4o-mini",
            "api_key": self.api_key,
            "base_url": self.base_url,
            "temperature": 0.3,
            "max_tokens": 1000
        }
        
        # åˆ›å»ºä»£ç†
        agents = []
        for i in range(max(agent_count, 2)):  # AutoGenéœ€è¦è‡³å°‘2ä¸ªä»£ç†
            agent = ConversableAgent(
                name=f"agent_{i}",
                llm_config=llm_config,
                system_message="You are an expert with specialized knowledge and analytical capabilities."
            )
            agents.append(agent)
        
        # åˆ›å»ºç¾¤èŠ
        group_chat = GroupChat(
            agents=agents,
            messages=[],
            max_round=5,
            speaker_selection_method='round_robin'
        )
        
        manager = GroupChatManager(
            groupchat=group_chat,
            llm_config=llm_config
        )
        
        # æ‰§è¡Œä»»åŠ¡
        successful_tasks = 0
        for i, task_prompt in enumerate(tasks):
            try:
                result = agents[0].initiate_chat(
                    manager,
                    message=task_prompt,
                    max_turns=3
                )
                if result and len(str(result).strip()) > 100:
                    successful_tasks += 1
            except Exception as e:
                logger.warning(f"AutoGenä»»åŠ¡{i}å¤±è´¥: {e}")
        
        return {"successful_tasks": successful_tasks}
    
    def run_scalability_study(self) -> Dict[str, Any]:
        """è¿è¡Œå¯æ‰©å±•æ€§å®éªŒ"""
        logger.info("ğŸ“ˆ å¼€å§‹å¯æ‰©å±•æ€§å®éªŒ...")
        logger.info(f"å®éªŒé…ç½®: {len(self.scalability_config['frameworks'])}ä¸ªæ¡†æ¶, "
                   f"ä»£ç†è§„æ¨¡: {len(self.scalability_config['agent_scales'])}, "
                   f"ä»»åŠ¡è§„æ¨¡: {len(self.scalability_config['task_scales'])}")
        
        scalability_results = []
        total_tests = (len(self.scalability_config['frameworks']) * 
                      len(self.scalability_config['agent_scales']) * 
                      len(self.scalability_config['task_scales']) * 
                      len(self.scalability_config['complexity_levels']) * 
                      self.scalability_config['repetitions'])
        
        current_test = 0
        
        for framework in self.scalability_config['frameworks']:
            for agent_scale in self.scalability_config['agent_scales']:
                for task_scale in self.scalability_config['task_scales']:
                    for complexity in self.scalability_config['complexity_levels']:
                        for repetition in range(self.scalability_config['repetitions']):
                            current_test += 1
                            logger.info(f"ğŸ“Š æµ‹è¯•è¿›åº¦: {current_test}/{total_tests} - "
                                       f"{framework} - {agent_scale} agents - {task_scale} tasks - {complexity}")
                            
                            # è¿è¡Œæµ‹è¯•
                            result = self.test_framework_scalability(
                                framework, "agent_scale", agent_scale, complexity, task_scale
                            )
                            
                            # æ·»åŠ é‡å¤æ¬¡æ•°ä¿¡æ¯
                            result["repetition"] = repetition + 1
                            scalability_results.append(result)
                            
                            # è®°å½•ç»“æœ
                            if result["status"] == "success":
                                logger.info(f"   âœ… æˆåŠŸ: ååé‡={result['throughput']:.3f} tasks/sec, "
                                           f"å»¶è¿Ÿ={result['avg_latency']:.2f} ms, "
                                           f"å†…å­˜={result['memory_usage']:.2f} MB")
                            else:
                                logger.error(f"   âŒ å¤±è´¥: {result.get('error', 'Unknown error')}")
                            
                            # çŸ­æš‚ä¼‘æ¯ä»¥é¿å…APIé™åˆ¶
                            time.sleep(1)
        
        # ä¿å­˜ç»“æœ
        results_file = "scalability_study_results.json"
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump({
                "scalability_results": scalability_results,
                "memory_tracker": self.memory_tracker,
                "timestamp": datetime.now().isoformat(),
                "scalability_config": self.scalability_config,
                "test_info": {
                    "total_tests": len(scalability_results),
                    "frameworks": self.scalability_config['frameworks'],
                    "agent_scales": self.scalability_config['agent_scales'],
                    "task_scales": self.scalability_config['task_scales'],
                    "complexity_levels": self.scalability_config['complexity_levels'],
                    "repetitions": self.scalability_config['repetitions'],
                    "random_seed": self.random_seed
                }
            }, f, indent=2, ensure_ascii=False)
        
        logger.info(f"âœ… å¯æ‰©å±•æ€§å®éªŒå®Œæˆï¼ç»“æœå·²ä¿å­˜åˆ°: {results_file}")
        
        # ç”Ÿæˆå¯æ‰©å±•æ€§åˆ†æ
        self._generate_scalability_analysis(scalability_results)
        
        return {
            "scalability_results": scalability_results,
            "memory_tracker": self.memory_tracker,
            "timestamp": datetime.now().isoformat(),
            "scalability_config": self.scalability_config
        }
    
    def _generate_scalability_analysis(self, scalability_results: List[Dict[str, Any]]):
        """ç”Ÿæˆå¯æ‰©å±•æ€§åˆ†æ"""
        logger.info("\nğŸ“Š å¯æ‰©å±•æ€§åˆ†æ:")
        logger.info("=" * 60)
        
        # æŒ‰æ¡†æ¶åˆ†ç»„ç»Ÿè®¡
        framework_stats = {}
        for result in scalability_results:
            if result["status"] == "success":
                framework = result["framework"]
                scale_value = result["scale_value"]
                
                if framework not in framework_stats:
                    framework_stats[framework] = {}
                
                if scale_value not in framework_stats[framework]:
                    framework_stats[framework][scale_value] = {
                        "throughput": [],
                        "latency": [],
                        "memory": [],
                        "success_rate": []
                    }
                
                framework_stats[framework][scale_value]["throughput"].append(result["throughput"])
                framework_stats[framework][scale_value]["latency"].append(result["avg_latency"])
                framework_stats[framework][scale_value]["memory"].append(result["memory_usage"])
                framework_stats[framework][scale_value]["success_rate"].append(result["success_rate"])
        
        # è¾“å‡ºå¯æ‰©å±•æ€§åˆ†æ
        for framework, scales in framework_stats.items():
            logger.info(f"\n{framework} å¯æ‰©å±•æ€§åˆ†æ:")
            logger.info("-" * 40)
            
            # è®¡ç®—ä¸åŒè§„æ¨¡ä¸‹çš„å¹³å‡æ€§èƒ½
            scale_performance = []
            for scale_value in sorted(scales.keys()):
                stats = scales[scale_value]
                if stats["throughput"]:
                    avg_throughput = np.mean(stats["throughput"])
                    avg_latency = np.mean(stats["latency"])
                    avg_memory = np.mean(stats["memory"])
                    avg_success_rate = np.mean(stats["success_rate"])
                    
                    scale_performance.append({
                        "scale": scale_value,
                        "throughput": avg_throughput,
                        "latency": avg_latency,
                        "memory": avg_memory,
                        "success_rate": avg_success_rate
                    })
                    
                    logger.info(f"  {scale_value}ä¸ªä»£ç†: ååé‡={avg_throughput:.3f} tasks/sec, "
                               f"å»¶è¿Ÿ={avg_latency:.2f} ms, å†…å­˜={avg_memory:.2f} MB")
            
            # åˆ†æå¯æ‰©å±•æ€§è¶‹åŠ¿
            if len(scale_performance) >= 3:
                # è®¡ç®—ååé‡æ‰©å±•æ€§
                throughputs = [p["throughput"] for p in scale_performance]
                latency_increase = (latencies[-1] - latencies[0]) / latencies[0] * 100 if len(latencies) > 1 else 0
                memory_increase = (memories[-1] - memories[0]) / memories[0] * 100 if len(memories) > 1 else 0
                
                logger.info(f"\n  å¯æ‰©å±•æ€§æŒ‡æ ‡:")
                logger.info(f"    ååé‡å˜åŒ–: {throughput_change:+.1f}%")
                logger.info(f"    å»¶è¿Ÿå¢åŠ : {latency_increase:+.1f}%")
                logger.info(f"    å†…å­˜å¢åŠ : {memory_increase:+.1f}%")
                
                # åˆ¤æ–­å¯æ‰©å±•æ€§ç­‰çº§
                if throughput_change > 50 and latency_increase < 100:
                    scalability_grade = "ä¼˜ç§€"
                elif throughput_change > 20 and latency_increase < 200:
                    scalability_grade = "è‰¯å¥½"
                elif throughput_change > 0 and latency_increase < 300:
                    scalability_grade = "ä¸€èˆ¬"
                else:
                    scalability_grade = "è¾ƒå·®"
                
                logger.info(f"    å¯æ‰©å±•æ€§ç­‰çº§: {scalability_grade}")
        
        # æ€»ä½“ç»Ÿè®¡
        total_tests = len(scalability_results)
        successful_tests = sum(1 for r in scalability_results if r["status"] == "success")
        overall_success_rate = (successful_tests / total_tests) * 100
        
        logger.info(f"\nğŸ“ˆ æ€»ä½“ç»Ÿè®¡:")
        logger.info(f"  æˆåŠŸæµ‹è¯•: {successful_tests}")
        logger.info(f"  å¤±è´¥æµ‹è¯•: {total_tests - successful_tests}")
        logger.info(f"  æˆåŠŸç‡: {overall_success_rate:.1f}%")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ“ˆ å¯æ‰©å±•æ€§å®éªŒæ¡†æ¶")
    print("=" * 50)
    print("âš ï¸  æ³¨æ„ï¼šæ­¤å®éªŒä½¿ç”¨çœŸå®APIè°ƒç”¨ï¼Œéœ€è¦æœ‰æ•ˆçš„APIå¯†é’¥")
    print("âš ï¸  å®éªŒè§„æ¨¡è¾ƒå¤§ï¼Œé¢„è®¡éœ€è¦è¾ƒé•¿æ—¶é—´å®Œæˆ")
    print("=" * 50)
    
    # åˆ›å»ºå¯æ‰©å±•æ€§å®éªŒå®ä¾‹
    scalability = ScalabilityStudy()
    
    # è¿è¡Œå¯æ‰©å±•æ€§å®éªŒ
    results = scalability.run_scalability_study()
    
    print("\nğŸ‰ å¯æ‰©å±•æ€§å®éªŒå®Œæˆï¼")
    print(f"æ€»æµ‹è¯•æ•°: {len(results['scalability_results'])}")
    print("ç»“æœå·²ä¿å­˜åˆ° scalability_study_results.json")

if __name__ == "__main__":
    main()
