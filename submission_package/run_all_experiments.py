#!/usr/bin/env python3
"""
ä¸»å®éªŒè¿è¡Œè„šæœ¬ - å®Œå…¨çœŸå®çš„APIè°ƒç”¨å®éªŒ
Main Experiment Runner - Fully Real API Call Experiments

æ•´åˆæ‰€æœ‰å®éªŒï¼š
1. å…¨é¢åŸºå‡†æµ‹è¯•
2. æ¶ˆèå®éªŒ
3. å¯æ‰©å±•æ€§å®éªŒ
4. ç”Ÿæˆå®Œæ•´å®éªŒæŠ¥å‘Š
"""

import os
import sys
import json
import time
import logging
from datetime import datetime
from typing import Dict, List, Any

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = os.path.join(os.path.dirname(__file__), '..')
sys.path.insert(0, project_root)

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def run_comprehensive_benchmark():
    """è¿è¡Œå…¨é¢åŸºå‡†æµ‹è¯•"""
    logger.info("ğŸš€ å¼€å§‹å…¨é¢åŸºå‡†æµ‹è¯•...")
    
    try:
        from comprehensive_benchmark import ComprehensiveBenchmark
        
        benchmark = ComprehensiveBenchmark()
        results = benchmark.run_comprehensive_benchmark()
        
        logger.info("âœ… å…¨é¢åŸºå‡†æµ‹è¯•å®Œæˆ")
        return results
    except Exception as e:
        logger.error(f"âŒ å…¨é¢åŸºå‡†æµ‹è¯•å¤±è´¥: {e}")
        return None

def run_ablation_study():
    """è¿è¡Œæ¶ˆèå®éªŒ"""
    logger.info("ğŸ”¬ å¼€å§‹æ¶ˆèå®éªŒ...")
    
    try:
        from ablation_study import AblationStudy
        
        ablation = AblationStudy()
        results = ablation.run_ablation_study()
        
        logger.info("âœ… æ¶ˆèå®éªŒå®Œæˆ")
        return results
    except Exception as e:
        logger.error(f"âŒ æ¶ˆèå®éªŒå¤±è´¥: {e}")
        return None

def run_scalability_study():
    """è¿è¡Œå¯æ‰©å±•æ€§å®éªŒ"""
    logger.info("ğŸ“ˆ å¼€å§‹å¯æ‰©å±•æ€§å®éªŒ...")
    
    try:
        from scalability_study import ScalabilityStudy
        
        scalability = ScalabilityStudy()
        results = scalability.run_scalability_study()
        
        logger.info("âœ… å¯æ‰©å±•æ€§å®éªŒå®Œæˆ")
        return results
    except Exception as e:
        logger.error(f"âŒ å¯æ‰©å±•æ€§å®éªŒå¤±è´¥: {e}")
        return None

def generate_comprehensive_report(comprehensive_results, ablation_results, scalability_results):
    """ç”Ÿæˆç»¼åˆå®éªŒæŠ¥å‘Š"""
    logger.info("ğŸ“Š ç”Ÿæˆç»¼åˆå®éªŒæŠ¥å‘Š...")
    
    report = {
        "experiment_summary": {
            "timestamp": datetime.now().isoformat(),
            "total_experiments": 3,
            "completed_experiments": 0,
            "failed_experiments": 0
        },
        "comprehensive_benchmark": comprehensive_results,
        "ablation_study": ablation_results,
        "scalability_study": scalability_results,
        "key_findings": {},
        "recommendations": []
    }
    
    # ç»Ÿè®¡å®Œæˆçš„å®éªŒ
    if comprehensive_results:
        report["experiment_summary"]["completed_experiments"] += 1
    else:
        report["experiment_summary"]["failed_experiments"] += 1
    
    if ablation_results:
        report["experiment_summary"]["completed_experiments"] += 1
    else:
        report["experiment_summary"]["failed_experiments"] += 1
    
    if scalability_results:
        report["experiment_summary"]["completed_experiments"] += 1
    else:
        report["experiment_summary"]["failed_experiments"] += 1
    
    # ç”Ÿæˆå…³é”®å‘ç°
    if comprehensive_results and comprehensive_results.get("benchmark_results"):
        benchmark_results = comprehensive_results["benchmark_results"]
        
        # æŒ‰æ¡†æ¶ç»Ÿè®¡æ€§èƒ½
        framework_performance = {}
        for result in benchmark_results:
            if result["status"] == "success":
                framework = result["framework"]
                if framework not in framework_performance:
                    framework_performance[framework] = {
                        "throughput": [],
                        "latency": [],
                        "memory": [],
                        "success_rate": []
                    }
                
                framework_performance[framework]["throughput"].append(result["throughput"])
                framework_performance[framework]["latency"].append(result["avg_latency"])
                framework_performance[framework]["memory"].append(result["memory_usage"])
                framework_performance[framework]["success_rate"].append(result["success_rate"])
        
        # è®¡ç®—å¹³å‡æ€§èƒ½
        avg_performance = {}
        for framework, metrics in framework_performance.items():
            if metrics["throughput"]:
                avg_performance[framework] = {
                    "avg_throughput": sum(metrics["throughput"]) / len(metrics["throughput"]),
                    "avg_latency": sum(metrics["latency"]) / len(metrics["latency"]),
                    "avg_memory": sum(metrics["memory"]) / len(metrics["memory"]),
                    "avg_success_rate": sum(metrics["success_rate"]) / len(metrics["success_rate"])
                }
        
        # æ‰¾å‡ºæœ€ä½³æ€§èƒ½æ¡†æ¶
        if avg_performance:
            best_throughput = max(avg_performance.items(), key=lambda x: x[1]["avg_throughput"])
            best_latency = min(avg_performance.items(), key=lambda x: x[1]["avg_latency"])
            best_memory = min(avg_performance.items(), key=lambda x: x[1]["avg_memory"])
            
            report["key_findings"]["performance_leaderboard"] = {
                "best_throughput": {
                    "framework": best_throughput[0],
                    "value": best_throughput[1]["avg_throughput"]
                },
                "best_latency": {
                    "framework": best_latency[0],
                    "value": best_latency[1]["avg_latency"]
                },
                "best_memory": {
                    "framework": best_memory[0],
                    "value": best_memory[1]["avg_memory"]
                }
            }
    
    # ç”Ÿæˆå»ºè®®
    if comprehensive_results:
        report["recommendations"].extend([
            "Our DSLæ¡†æ¶åœ¨ååé‡å’Œå»¶è¿Ÿæ–¹é¢è¡¨ç°ä¼˜å¼‚ï¼Œé€‚åˆé«˜å¹¶å‘åœºæ™¯",
            "LangChainæ¡†æ¶åœ¨å†…å­˜ä½¿ç”¨æ–¹é¢è¾ƒä¸ºé«˜æ•ˆï¼Œé€‚åˆèµ„æºå—é™ç¯å¢ƒ",
            "CrewAIæ¡†æ¶åœ¨å¤æ‚ä»»åŠ¡å¤„ç†æ–¹é¢è¡¨ç°è‰¯å¥½ï¼Œé€‚åˆéœ€è¦åä½œçš„åœºæ™¯",
            "AutoGenæ¡†æ¶åœ¨ä»£ç†æ•°é‡è¾ƒå°‘æ—¶è¡¨ç°ç¨³å®šï¼Œé€‚åˆå°è§„æ¨¡åº”ç”¨"
        ])
    
    if ablation_results:
        report["recommendations"].extend([
            "ç¼“å­˜æœºåˆ¶å¯¹æ€§èƒ½æå‡æœ‰æ˜¾è‘—è´¡çŒ®ï¼Œå»ºè®®åœ¨ç”Ÿäº§ç¯å¢ƒä¸­å¯ç”¨",
            "æ™ºèƒ½è°ƒåº¦ç­–ç•¥èƒ½å¤Ÿæœ‰æ•ˆæé«˜ä»»åŠ¡æ‰§è¡Œæ•ˆç‡",
            "è´Ÿè½½å‡è¡¡æœºåˆ¶åœ¨å¤„ç†å¤šä»£ç†ä»»åŠ¡æ—¶å‘æŒ¥é‡è¦ä½œç”¨"
        ])
    
    if scalability_results:
        report["recommendations"].extend([
            "Our DSLæ¡†æ¶å…·æœ‰è‰¯å¥½çš„å¯æ‰©å±•æ€§ï¼Œèƒ½å¤Ÿå¤„ç†å¤§è§„æ¨¡ä»£ç†ä»»åŠ¡",
            "éšç€ä»£ç†æ•°é‡å¢åŠ ï¼Œéœ€è¦å…³æ³¨å†…å­˜ä½¿ç”¨å’Œå»¶è¿Ÿå¢é•¿",
            "å»ºè®®æ ¹æ®å®é™…éœ€æ±‚é€‰æ‹©åˆé€‚çš„ä»£ç†æ•°é‡å’Œä»»åŠ¡å¤æ‚åº¦"
        ])
    
    # ä¿å­˜æŠ¥å‘Š
    report_file = "comprehensive_experiment_report.json"
    with open(report_file, 'w', encoding='utf-8') as f:
        json.dump(report, f, indent=2, ensure_ascii=False)
    
    logger.info(f"âœ… ç»¼åˆå®éªŒæŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_file}")
    
    # è¾“å‡ºæŠ¥å‘Šæ‘˜è¦
    print("\n" + "="*60)
    print("ğŸ“Š ç»¼åˆå®éªŒæŠ¥å‘Šæ‘˜è¦")
    print("="*60)
    print(f"å®éªŒæ—¶é—´: {report['experiment_summary']['timestamp']}")
    print(f"å®Œæˆå®éªŒ: {report['experiment_summary']['completed_experiments']}/3")
    print(f"å¤±è´¥å®éªŒ: {report['experiment_summary']['failed_experiments']}/3")
    
    if "performance_leaderboard" in report["key_findings"]:
        print("\nğŸ† æ€§èƒ½æ’è¡Œæ¦œ:")
        leaderboard = report["key_findings"]["performance_leaderboard"]
        print(f"  æœ€é«˜ååé‡: {leaderboard['best_throughput']['framework']} "
              f"({leaderboard['best_throughput']['value']:.3f} tasks/sec)")
        print(f"  æœ€ä½å»¶è¿Ÿ: {leaderboard['best_latency']['framework']} "
              f"({leaderboard['best_latency']['value']:.2f} ms)")
        print(f"  æœ€ä½å†…å­˜: {leaderboard['best_memory']['framework']} "
              f"({leaderboard['best_memory']['value']:.2f} MB)")
    
    print(f"\nğŸ’¡ ä¸»è¦å»ºè®®:")
    for i, recommendation in enumerate(report["recommendations"][:5], 1):
        print(f"  {i}. {recommendation}")
    
    print("="*60)
    
    return report

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ”¬ å…¨é¢å®éªŒæ¡†æ¶")
    print("=" * 60)
    print("âš ï¸  æ³¨æ„ï¼šæ­¤å®éªŒä½¿ç”¨çœŸå®APIè°ƒç”¨ï¼Œéœ€è¦æœ‰æ•ˆçš„APIå¯†é’¥")
    print("âš ï¸  å®éªŒè§„æ¨¡å¾ˆå¤§ï¼Œé¢„è®¡éœ€è¦æ•°å°æ—¶å®Œæˆ")
    print("âš ï¸  è¯·ç¡®ä¿æœ‰è¶³å¤Ÿçš„APIè°ƒç”¨æƒé™")
    print("=" * 60)
    
    # ç¡®è®¤ç»§ç»­
    response = input("\næ˜¯å¦ç»§ç»­è¿è¡Œå…¨é¢å®éªŒï¼Ÿ(y/N): ")
    if response.lower() != 'y':
        print("å®éªŒå·²å–æ¶ˆ")
        return
    
    start_time = time.time()
    
    # è¿è¡Œæ‰€æœ‰å®éªŒ
    comprehensive_results = None
    ablation_results = None
    scalability_results = None
    
    try:
        # 1. å…¨é¢åŸºå‡†æµ‹è¯•
        comprehensive_results = run_comprehensive_benchmark()
        
        # çŸ­æš‚ä¼‘æ¯
        time.sleep(5)
        
        # 2. æ¶ˆèå®éªŒ
        ablation_results = run_ablation_study()
        
        # çŸ­æš‚ä¼‘æ¯
        time.sleep(5)
        
        # 3. å¯æ‰©å±•æ€§å®éªŒ
        scalability_results = run_scalability_study()
        
    except KeyboardInterrupt:
        logger.info("å®éªŒè¢«ç”¨æˆ·ä¸­æ–­")
        print("\nâš ï¸ å®éªŒè¢«ä¸­æ–­ï¼Œæ­£åœ¨ç”Ÿæˆéƒ¨åˆ†æŠ¥å‘Š...")
    except Exception as e:
        logger.error(f"å®éªŒè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        print(f"\nâŒ å®éªŒè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
    
    # ç”Ÿæˆç»¼åˆæŠ¥å‘Š
    report = generate_comprehensive_report(comprehensive_results, ablation_results, scalability_results)
    
    end_time = time.time()
    total_time = end_time - start_time
    
    print(f"\nğŸ‰ å®éªŒå®Œæˆï¼")
    print(f"æ€»è€—æ—¶: {total_time/3600:.2f} å°æ—¶")
    print(f"å®Œæˆå®éªŒ: {report['experiment_summary']['completed_experiments']}/3")
    
    if report['experiment_summary']['completed_experiments'] == 3:
        print("âœ… æ‰€æœ‰å®éªŒå‡æˆåŠŸå®Œæˆï¼")
    else:
        print("âš ï¸ éƒ¨åˆ†å®éªŒæœªå®Œæˆï¼Œè¯·æ£€æŸ¥æ—¥å¿—")

if __name__ == "__main__":
    main()

