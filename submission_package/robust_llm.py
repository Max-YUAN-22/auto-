#!/usr/bin/env python3
"""
ä¿®å¤APIè°ƒç”¨é—®é¢˜çš„å¢å¼ºç‰ˆLLMæ¨¡å—
Enhanced LLM Module with Fixed API Calls
"""

import os
import json
import logging
import time
import random
from functools import lru_cache
from typing import Optional, Dict, Any
from openai import OpenAI
import requests

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class RobustLLMClient:
    """å¢å¼ºçš„LLMå®¢æˆ·ç«¯ï¼Œå…·æœ‰é‡è¯•æœºåˆ¶å’Œé™çº§ç­–ç•¥"""
    
    def __init__(self, api_key: str = None, base_url: str = None, model: str = "deepseek-chat"):
        self.api_key = api_key or os.getenv("DEEPSEEK_API_KEY", "").strip()
        self.base_url = base_url or "https://api.deepseek.com"
        self.model = model
        self.client = None
        self.fallback_responses = self._init_fallback_responses()
        self.request_count = 0
        self.last_request_time = 0
        self.rate_limit_delay = 1.0  # è¯·æ±‚é—´éš”
        
        if self.api_key:
            try:
                self.client = OpenAI(
                    api_key=self.api_key,
                    base_url=self.base_url,
                    timeout=30,
                    max_retries=2
                )
                logger.info("LLMå®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸ")
            except Exception as e:
                logger.error(f"LLMå®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥: {e}")
                self.client = None
        else:
            logger.warning("æœªè®¾ç½®DEEPSEEK_API_KEYï¼Œå°†ä½¿ç”¨é™çº§ç­–ç•¥")
    
    def _init_fallback_responses(self) -> Dict[str, str]:
        """åˆå§‹åŒ–é™çº§å“åº”æ¨¡æ¿"""
        return {
            "math": "è®¡ç®—ç»“æœ: 42",
            "text": "æ–‡æœ¬å¤„ç†å®Œæˆ",
            "analysis": "æ•°æ®åˆ†æå®Œæˆï¼Œå‘ç°3ä¸ªå…³é”®æ¨¡å¼",
            "decision": "å†³ç­–å»ºè®®: ç»§ç»­å½“å‰ç­–ç•¥",
            "default": "ä»»åŠ¡å¤„ç†å®Œæˆ"
        }
    
    def _get_fallback_response(self, prompt: str) -> str:
        """è·å–é™çº§å“åº”"""
        prompt_lower = prompt.lower()
        
        if any(word in prompt_lower for word in ["è®¡ç®—", "æ•°å­¦", "math", "calculate"]):
            return self.fallback_responses["math"]
        elif any(word in prompt_lower for word in ["æ–‡æœ¬", "text", "å¤„ç†", "process"]):
            return self.fallback_responses["text"]
        elif any(word in prompt_lower for word in ["åˆ†æ", "analysis", "æ•°æ®", "data"]):
            return self.fallback_responses["analysis"]
        elif any(word in prompt_lower for word in ["å†³ç­–", "decision", "é€‰æ‹©", "choose"]):
            return self.fallback_responses["decision"]
        else:
            return self.fallback_responses["default"]
    
    def _rate_limit_control(self):
        """æ§åˆ¶è¯·æ±‚é¢‘ç‡"""
        current_time = time.time()
        time_since_last = current_time - self.last_request_time
        
        if time_since_last < self.rate_limit_delay:
            sleep_time = self.rate_limit_delay - time_since_last
            time.sleep(sleep_time)
        
        self.last_request_time = time.time()
    
    def call_with_retry(self, prompt: str, role: str = None, max_retries: int = 3) -> str:
        """å¸¦é‡è¯•æœºåˆ¶çš„LLMè°ƒç”¨"""
        
        # å¦‚æœæ²¡æœ‰APIå¯†é’¥æˆ–å®¢æˆ·ç«¯ï¼Œç›´æ¥ä½¿ç”¨é™çº§ç­–ç•¥
        if not self.api_key or not self.client:
            logger.info("ä½¿ç”¨é™çº§ç­–ç•¥")
            return self._get_fallback_response(prompt)
        
        # æ§åˆ¶è¯·æ±‚é¢‘ç‡
        self._rate_limit_control()
        
        for attempt in range(max_retries):
            try:
                logger.info(f"LLMè°ƒç”¨å°è¯• {attempt + 1}/{max_retries}")
                
                completion = self.client.chat.completions.create(
                    model=self.model,
                    messages=[
                        {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½åŸå¸‚ç®¡ç†åŠ©æ‰‹ï¼Œè´Ÿè´£å¤„ç†å„ç§åŸå¸‚è¿è¥ä»»åŠ¡ã€‚è¯·ç”¨ä¸­æ–‡ç®€æ´åœ°å›åº”ç”¨æˆ·çš„è¯·æ±‚ã€‚"},
                        {"role": "user", "content": prompt}
                    ],
                    temperature=0.3,
                    max_tokens=500,
                    timeout=30
                )
                
                response = completion.choices[0].message.content
                if response and response.strip():
                    logger.info("LLMè°ƒç”¨æˆåŠŸ")
                    return response
                else:
                    logger.warning("æ”¶åˆ°ç©ºå“åº”")
                    if attempt == max_retries - 1:
                        return self._get_fallback_response(prompt)
                    
            except Exception as e:
                logger.error(f"LLMè°ƒç”¨å¤±è´¥ (å°è¯• {attempt + 1}): {e}")
                
                # æ£€æŸ¥æ˜¯å¦æ˜¯é€Ÿç‡é™åˆ¶é”™è¯¯
                if "rate limit" in str(e).lower() or "429" in str(e):
                    wait_time = (2 ** attempt) + random.uniform(0, 1)
                    logger.info(f"é€Ÿç‡é™åˆ¶ï¼Œç­‰å¾… {wait_time:.2f} ç§’")
                    time.sleep(wait_time)
                    continue
                
                # æ£€æŸ¥æ˜¯å¦æ˜¯è®¤è¯é”™è¯¯
                if "401" in str(e) or "unauthorized" in str(e).lower():
                    logger.error("APIå¯†é’¥æ— æ•ˆï¼Œåˆ‡æ¢åˆ°é™çº§ç­–ç•¥")
                    return self._get_fallback_response(prompt)
                
                # å…¶ä»–é”™è¯¯ï¼Œç­‰å¾…åé‡è¯•
                if attempt < max_retries - 1:
                    wait_time = (2 ** attempt) + random.uniform(0, 1)
                    logger.info(f"ç­‰å¾… {wait_time:.2f} ç§’åé‡è¯•")
                    time.sleep(wait_time)
        
        # æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥ï¼Œä½¿ç”¨é™çº§ç­–ç•¥
        logger.warning("æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥ï¼Œä½¿ç”¨é™çº§ç­–ç•¥")
        return self._get_fallback_response(prompt)

# å…¨å±€LLMå®¢æˆ·ç«¯å®ä¾‹
_llm_client = None

def get_robust_llm_client() -> RobustLLMClient:
    """è·å–å…¨å±€LLMå®¢æˆ·ç«¯å®ä¾‹"""
    global _llm_client
    if _llm_client is None:
        _llm_client = RobustLLMClient()
    return _llm_client

def llm_callable(prompt: str, role: str = None) -> str:
    """
    å¢å¼ºçš„LLMè°ƒç”¨å‡½æ•°ï¼Œå…·æœ‰é‡è¯•æœºåˆ¶å’Œé™çº§ç­–ç•¥
    """
    client = get_robust_llm_client()
    return client.call_with_retry(prompt, role)

def test_api_connection() -> Dict[str, Any]:
    """æµ‹è¯•APIè¿æ¥"""
    client = get_robust_llm_client()
    
    test_prompt = "è¯·å›ç­”ï¼š1+1ç­‰äºå¤šå°‘ï¼Ÿ"
    
    start_time = time.time()
    response = client.call_with_retry(test_prompt)
    end_time = time.time()
    
    return {
        "success": True,
        "response": response,
        "latency": end_time - start_time,
        "api_key_set": bool(client.api_key),
        "client_available": bool(client.client)
    }

def main():
    """æµ‹è¯•å‡½æ•°"""
    print("ğŸ”§ æµ‹è¯•å¢å¼ºçš„LLMå®¢æˆ·ç«¯...")
    
    # æµ‹è¯•APIè¿æ¥
    result = test_api_connection()
    
    print(f"âœ… APIè¿æ¥æµ‹è¯•ç»“æœ:")
    print(f"   æˆåŠŸ: {result['success']}")
    print(f"   å“åº”: {result['response']}")
    print(f"   å»¶è¿Ÿ: {result['latency']:.3f}ç§’")
    print(f"   APIå¯†é’¥å·²è®¾ç½®: {result['api_key_set']}")
    print(f"   å®¢æˆ·ç«¯å¯ç”¨: {result['client_available']}")
    
    # æµ‹è¯•å¤šä¸ªè°ƒç”¨
    print(f"\nğŸ”„ æµ‹è¯•å¤šä¸ªè°ƒç”¨...")
    test_prompts = [
        "è®¡ç®— 2+2",
        "åˆ†ææ•°æ®",
        "åšå†³ç­–",
        "å¤„ç†æ–‡æœ¬"
    ]
    
    for prompt in test_prompts:
        response = llm_callable(prompt)
        print(f"   è¾“å…¥: {prompt}")
        print(f"   è¾“å‡º: {response}")
        print()

if __name__ == "__main__":
    main()
