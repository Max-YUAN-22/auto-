#!/usr/bin/env python3
"""
å…¨é¢åŸºå‡†æµ‹è¯•æ¡†æ¶ - å®Œå…¨çœŸå®çš„APIè°ƒç”¨å®éªŒ
Comprehensive Benchmark Framework - Fully Real API Call Experiments

åŒ…å«ï¼š
1. å¤šåœºæ™¯å¯¹æ¯”å®éªŒ
2. æ¶ˆèå®éªŒ  
3. å¯æ‰©å±•æ€§å®éªŒ
4. é²æ£’æ€§å®éªŒ
5. å†…å­˜æ•ˆç‡å®éªŒ
6. å»¶è¿Ÿåˆ†æå®éªŒ
"""

import os
import sys
import json
import time
import logging
import numpy as np
import psutil
import gc
from datetime import datetime
from typing import Dict, List, Any, Tuple
from contextlib import contextmanager
import subprocess

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = os.path.join(os.path.dirname(__file__), '..')
sys.path.insert(0, project_root)

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class ComprehensiveBenchmark:
    """å…¨é¢åŸºå‡†æµ‹è¯•ç±»"""
    
    def __init__(self, random_seed: int = 42):
        self.random_seed = random_seed
        np.random.seed(random_seed)
        
        # åŠ è½½ç¯å¢ƒå˜é‡
        self.load_env()
        
        # è®¾ç½®APIé…ç½®
        self.api_key = os.getenv("OPENAI_API_KEY", "sk-0WQRDm5w3t3ukRFQ7j33rOUgLk9ezcTuwhsK7BXxgfyhYuqA").strip()
        self.base_url = "https://www.yunqiaoai.top/v1"
        
        # å†…å­˜è·Ÿè¸ªå™¨
        self.memory_tracker = {}
        
        # å®éªŒé…ç½®
        self.experiment_config = {
            "scenarios": [
                "business_analysis",      # å•†ä¸šåˆ†æ
                "technical_design",       # æŠ€æœ¯è®¾è®¡
                "scientific_research",    # ç§‘å­¦ç ”ç©¶
                "creative_writing",       # åˆ›æ„å†™ä½œ
                "data_analysis",          # æ•°æ®åˆ†æ
                "system_optimization",    # ç³»ç»Ÿä¼˜åŒ–
                "risk_assessment",        # é£é™©è¯„ä¼°
                "strategic_planning"      # æˆ˜ç•¥è§„åˆ’
            ],
            "agent_counts": [1, 2, 3, 4, 5, 6, 8, 10],  # æ›´å¤šä»£ç†æ•°é‡
            "frameworks": ["Our DSL", "LangChain", "CrewAI", "AutoGen"],
            "task_complexities": ["simple", "medium", "complex"],  # ä»»åŠ¡å¤æ‚åº¦
            "repetitions": 3  # æ¯ä¸ªå®éªŒé‡å¤3æ¬¡
        }
        
        # çœŸå®æµ‹è¯•ä»»åŠ¡ - 8ä¸ªä¸åŒåœºæ™¯ï¼Œæ¯ä¸ªåœºæ™¯3ä¸ªå¤æ‚åº¦
        self.tasks = self._generate_comprehensive_tasks()
        
        logger.info(f"åˆå§‹åŒ–å®Œæˆ - åœºæ™¯æ•°: {len(self.experiment_config['scenarios'])}, "
                   f"ä»£ç†æ•°é‡: {len(self.experiment_config['agent_counts'])}, "
                   f"ä»»åŠ¡æ€»æ•°: {sum(len(tasks) for tasks in self.tasks.values())}")
    
    def load_env(self):
        """åŠ è½½ç¯å¢ƒå˜é‡"""
        os.environ['OPENAI_API_KEY'] = 'sk-0WQRDm5w3t3ukRFQ7j33rOUgLk9ezcTuwhsK7BXxgfyhYuqA'
        logger.info("è®¾ç½®OPENAI_API_KEYç¯å¢ƒå˜é‡")
    
    def _generate_comprehensive_tasks(self) -> Dict[str, Dict[str, List[str]]]:
        """ç”Ÿæˆå…¨é¢çš„æµ‹è¯•ä»»åŠ¡"""
        tasks = {}
        
        # å•†ä¸šåˆ†æåœºæ™¯
        tasks["business_analysis"] = {
            "simple": [
                "åˆ†æä¸€å®¶å°å‹å’–å•¡åº—çš„æœˆåº¦é”€å”®æ•°æ®ï¼Œè¯†åˆ«é”€å”®è¶‹åŠ¿å¹¶æä¾›æ”¹è¿›å»ºè®®ã€‚",
                "è¯„ä¼°ä¸€ä¸ªåœ¨çº¿æ•™è‚²å¹³å°çš„ç”¨æˆ·å¢é•¿ç­–ç•¥ï¼Œæå‡ºä¼˜åŒ–æ–¹æ¡ˆã€‚"
            ],
            "medium": [
                "ä¸ºä¸€å®¶ä¸­å‹åˆ¶é€ ä¼ä¸šè®¾è®¡æ•°å­—åŒ–è½¬å‹è·¯çº¿å›¾ï¼ŒåŒ…æ‹¬æŠ€æœ¯é€‰å‹ã€å®æ–½è®¡åˆ’å’Œé£é™©è¯„ä¼°ã€‚",
                "åˆ†æç”µå•†å¹³å°çš„ä¾›åº”é“¾ç®¡ç†é—®é¢˜ï¼Œè®¾è®¡ä¼˜åŒ–æ–¹æ¡ˆä»¥æé«˜æ•ˆç‡å’Œé™ä½æˆæœ¬ã€‚"
            ],
            "complex": [
                "è®¾è®¡ä¸€ä¸ªå¤šå›½ä¼ä¸šçš„å…¨çƒä¾›åº”é“¾é£é™©ç®¡ç†æ¡†æ¶ï¼Œè€ƒè™‘åœ°ç¼˜æ”¿æ²»ã€è‡ªç„¶ç¾å®³ã€å¸‚åœºæ³¢åŠ¨ç­‰å› ç´ ï¼Œå¹¶æä¾›å®æ—¶ç›‘æ§å’Œåº”æ€¥å“åº”æœºåˆ¶ã€‚",
                "ä¸ºä¸€å®¶å¤§å‹é‡‘èæœºæ„è®¾è®¡åŸºäºAIçš„æ™ºèƒ½æŠ•é¡¾ç³»ç»Ÿæ¶æ„ï¼ŒåŒ…æ‹¬å®¢æˆ·ç”»åƒã€é£é™©è¯„ä¼°ã€æŠ•èµ„ç»„åˆä¼˜åŒ–å’Œåˆè§„ç®¡ç†ã€‚"
            ]
        }
        
        # æŠ€æœ¯è®¾è®¡åœºæ™¯
        tasks["technical_design"] = {
            "simple": [
                "è®¾è®¡ä¸€ä¸ªç®€å•çš„RESTful APIæ¶æ„ï¼ŒåŒ…æ‹¬è®¤è¯ã€æˆæƒå’Œæ•°æ®éªŒè¯æœºåˆ¶ã€‚",
                "ä¸ºç§»åŠ¨åº”ç”¨è®¾è®¡ç”¨æˆ·è®¤è¯å’Œä¼šè¯ç®¡ç†ç³»ç»Ÿã€‚"
            ],
            "medium": [
                "è®¾è®¡ä¸€ä¸ªå¾®æœåŠ¡æ¶æ„çš„ç”µå•†å¹³å°ï¼ŒåŒ…æ‹¬ç”¨æˆ·æœåŠ¡ã€å•†å“æœåŠ¡ã€è®¢å•æœåŠ¡å’Œæ”¯ä»˜æœåŠ¡ï¼Œè€ƒè™‘æœåŠ¡é—´é€šä¿¡ã€æ•°æ®ä¸€è‡´æ€§å’Œå®¹é”™æœºåˆ¶ã€‚",
                "è®¾è®¡ä¸€ä¸ªåˆ†å¸ƒå¼ç¼“å­˜ç³»ç»Ÿï¼Œæ”¯æŒé«˜å¹¶å‘è¯»å†™ã€æ•°æ®ä¸€è‡´æ€§å’Œæ•…éšœæ¢å¤ã€‚"
            ],
            "complex": [
                "è®¾è®¡ä¸€ä¸ªæ”¯æŒåƒä¸‡çº§ç”¨æˆ·çš„å®æ—¶æ¨èç³»ç»Ÿæ¶æ„ï¼ŒåŒ…æ‹¬æ•°æ®æ”¶é›†ã€ç‰¹å¾å·¥ç¨‹ã€æ¨¡å‹è®­ç»ƒã€åœ¨çº¿æ¨ç†å’ŒA/Bæµ‹è¯•æ¡†æ¶ï¼Œè€ƒè™‘å»¶è¿Ÿã€å‡†ç¡®æ€§å’Œå¯æ‰©å±•æ€§ã€‚",
                "è®¾è®¡ä¸€ä¸ªå¤šäº‘ç¯å¢ƒä¸‹çš„å®¹å™¨ç¼–æ’å¹³å°ï¼Œæ”¯æŒè‡ªåŠ¨æ‰©ç¼©å®¹ã€æœåŠ¡å‘ç°ã€è´Ÿè½½å‡è¡¡ã€ç›‘æ§å‘Šè­¦å’Œç¾éš¾æ¢å¤ã€‚"
            ]
        }
        
        # ç§‘å­¦ç ”ç©¶åœºæ™¯
        tasks["scientific_research"] = {
            "simple": [
                "åˆ†ææ°”å€™å˜åŒ–å¯¹å†œä¸šäº§é‡çš„å½±å“ï¼ŒåŸºäºå†å²æ•°æ®å»ºç«‹é¢„æµ‹æ¨¡å‹ã€‚",
                "ç ”ç©¶ç¤¾äº¤åª’ä½“å¯¹é’å°‘å¹´å¿ƒç†å¥åº·çš„å½±å“ï¼Œè®¾è®¡å®éªŒæ–¹æ¡ˆã€‚"
            ],
            "medium": [
                "è®¾è®¡ä¸€ä¸ªåŸºäºæœºå™¨å­¦ä¹ çš„è¯ç‰©å‘ç°å¹³å°ï¼ŒåŒ…æ‹¬åˆ†å­ç­›é€‰ã€æ´»æ€§é¢„æµ‹å’Œæ¯’æ€§è¯„ä¼°ï¼Œè€ƒè™‘æ•°æ®è´¨é‡å’Œæ¨¡å‹å¯è§£é‡Šæ€§ã€‚",
                "ç ”ç©¶é‡å­è®¡ç®—åœ¨å¯†ç å­¦ä¸­çš„åº”ç”¨ï¼Œåˆ†æé‡å­ç®—æ³•å¯¹ç°æœ‰åŠ å¯†ç³»ç»Ÿçš„å½±å“å¹¶æå‡ºåé‡å­å¯†ç å­¦æ–¹æ¡ˆã€‚"
            ],
            "complex": [
                "è®¾è®¡ä¸€ä¸ªå¤šæ¨¡æ€AIç³»ç»Ÿç”¨äºç™Œç—‡æ—©æœŸè¯Šæ–­ï¼Œæ•´åˆå½±åƒå­¦ã€åŸºå› ç»„å­¦ã€è›‹ç™½è´¨ç»„å­¦å’Œä¸´åºŠæ•°æ®ï¼Œå»ºç«‹ç«¯åˆ°ç«¯çš„è¯Šæ–­å’Œé¢„åé¢„æµ‹æ¨¡å‹ã€‚",
                "ç ”ç©¶å¯æ§æ ¸èšå˜ååº”å †çš„ç­‰ç¦»å­ä½“æ§åˆ¶ç®—æ³•ï¼Œè®¾è®¡å®æ—¶æ§åˆ¶ç³»ç»Ÿä»¥ç»´æŒç­‰ç¦»å­ä½“ç¨³å®šæ€§å’Œä¼˜åŒ–èƒ½é‡è¾“å‡ºã€‚"
            ]
        }
        
        # åˆ›æ„å†™ä½œåœºæ™¯
        tasks["creative_writing"] = {
            "simple": [
                "åˆ›ä½œä¸€ä¸ªå…³äºäººå·¥æ™ºèƒ½ä¸äººç±»åˆä½œçš„çŸ­ç¯‡ç§‘å¹»æ•…äº‹ã€‚",
                "ç¼–å†™ä¸€ä¸ªå…³äºç¯ä¿ä¸»é¢˜çš„å„¿ç«¥æ•™è‚²å‰§æœ¬ã€‚"
            ],
            "medium": [
                "åˆ›ä½œä¸€ä¸ªå¤šçº¿ç¨‹çš„æ‚¬ç–‘å°è¯´å¤§çº²ï¼ŒåŒ…å«å¤šä¸ªè§†è§’å’Œå¤æ‚çš„äººç‰©å…³ç³»ã€‚",
                "ç¼–å†™ä¸€ä¸ªå…³äºæœªæ¥åŸå¸‚ç”Ÿæ´»çš„äº’åŠ¨å¼æ•°å­—å™äº‹ä½œå“ã€‚"
            ],
            "complex": [
                "åˆ›ä½œä¸€ä¸ªè·¨åª’ä½“çš„ç§‘å¹»å²è¯—ï¼ŒåŒ…æ‹¬å°è¯´ã€æ¸¸æˆã€å½±è§†å’Œè™šæ‹Ÿç°å®ä½“éªŒï¼Œæ„å»ºå®Œæ•´çš„ä¸–ç•Œè§‚å’Œè§’è‰²ä½“ç³»ã€‚",
                "ç¼–å†™ä¸€ä¸ªåŸºäºå†å²äº‹ä»¶çš„æ²‰æµ¸å¼æˆå‰§ä½œå“ï¼Œèåˆä¼ ç»Ÿæˆå‰§ã€ç°ä»£æŠ€æœ¯å’Œè§‚ä¼—å‚ä¸å…ƒç´ ã€‚"
            ]
        }
        
        # æ•°æ®åˆ†æåœºæ™¯
        tasks["data_analysis"] = {
            "simple": [
                "åˆ†æç”µå•†å¹³å°çš„ç”¨æˆ·è¡Œä¸ºæ•°æ®ï¼Œè¯†åˆ«è´­ä¹°æ¨¡å¼å’Œç”¨æˆ·åå¥½ã€‚",
                "åˆ†æè‚¡ç¥¨å¸‚åœºçš„ä»·æ ¼æ³¢åŠ¨ï¼Œå»ºç«‹ç®€å•çš„é¢„æµ‹æ¨¡å‹ã€‚"
            ],
            "medium": [
                "åˆ†æåŸå¸‚äº¤é€šæ•°æ®ï¼Œè®¾è®¡æ™ºèƒ½äº¤é€šç®¡ç†ç³»ç»Ÿä»¥å‡å°‘æ‹¥å µå’Œæé«˜æ•ˆç‡ã€‚",
                "åˆ†æåŒ»ç–—æ•°æ®ï¼Œå»ºç«‹ç–¾ç—…é¢„æµ‹æ¨¡å‹å¹¶è¯„ä¼°å…¶ä¸´åºŠä»·å€¼ã€‚"
            ],
            "complex": [
                "åˆ†æå¤§è§„æ¨¡å¤šæºå¼‚æ„æ•°æ®ï¼ˆåŒ…æ‹¬æ–‡æœ¬ã€å›¾åƒã€ä¼ æ„Ÿå™¨æ•°æ®ï¼‰ï¼Œå»ºç«‹ç»¼åˆæ€§çš„åŸå¸‚æ™ºæ…§ç®¡ç†ç³»ç»Ÿï¼Œå®ç°å®æ—¶å†³ç­–å’Œé¢„æµ‹æ€§ç»´æŠ¤ã€‚",
                "åˆ†æé‡‘èå¸‚åœºçš„å¤šç»´åº¦æ•°æ®ï¼Œå»ºç«‹é«˜é¢‘äº¤æ˜“ç­–ç•¥å’Œé£é™©ç®¡ç†ç³»ç»Ÿï¼Œè€ƒè™‘å¸‚åœºå¾®è§‚ç»“æ„ã€æµåŠ¨æ€§é£é™©å’Œç›‘ç®¡åˆè§„ã€‚"
            ]
        }
        
        # ç³»ç»Ÿä¼˜åŒ–åœºæ™¯
        tasks["system_optimization"] = {
            "simple": [
                "ä¼˜åŒ–ä¸€ä¸ªWebåº”ç”¨çš„æ•°æ®åº“æŸ¥è¯¢æ€§èƒ½ï¼Œå‡å°‘å“åº”æ—¶é—´ã€‚",
                "ä¼˜åŒ–ä¸€ä¸ªç§»åŠ¨åº”ç”¨çš„ç”µæ± ä½¿ç”¨æ•ˆç‡ã€‚"
            ],
            "medium": [
                "ä¼˜åŒ–ä¸€ä¸ªåˆ†å¸ƒå¼ç³»ç»Ÿçš„èµ„æºåˆ©ç”¨ç‡ï¼ŒåŒ…æ‹¬CPUã€å†…å­˜ã€ç½‘ç»œå’Œå­˜å‚¨ï¼Œå®ç°åŠ¨æ€è´Ÿè½½å‡è¡¡å’Œèµ„æºè°ƒåº¦ã€‚",
                "ä¼˜åŒ–ä¸€ä¸ªæœºå™¨å­¦ä¹ è®­ç»ƒç®¡é“çš„æ•ˆç‡ï¼ŒåŒ…æ‹¬æ•°æ®é¢„å¤„ç†ã€æ¨¡å‹è®­ç»ƒå’Œè¶…å‚æ•°è°ƒä¼˜ã€‚"
            ],
            "complex": [
                "ä¼˜åŒ–ä¸€ä¸ªè¶…å¤§è§„æ¨¡äº‘è®¡ç®—å¹³å°çš„æ•´ä½“æ€§èƒ½ï¼ŒåŒ…æ‹¬è®¡ç®—ã€å­˜å‚¨ã€ç½‘ç»œã€å®‰å…¨å’Œæˆæœ¬ä¼˜åŒ–ï¼Œå®ç°å¤šç§Ÿæˆ·éš”ç¦»å’Œå¼¹æ€§æ‰©ç¼©å®¹ã€‚",
                "ä¼˜åŒ–ä¸€ä¸ªè‡ªåŠ¨é©¾é©¶ç³»ç»Ÿçš„å®æ—¶æ€§èƒ½ï¼ŒåŒ…æ‹¬æ„ŸçŸ¥ã€å†³ç­–ã€æ§åˆ¶å’Œé€šä¿¡ï¼Œç¡®ä¿å®‰å…¨æ€§å’Œæ•ˆç‡çš„å¹³è¡¡ã€‚"
            ]
        }
        
        # é£é™©è¯„ä¼°åœºæ™¯
        tasks["risk_assessment"] = {
            "simple": [
                "è¯„ä¼°ä¸€ä¸ªåˆ›ä¸šé¡¹ç›®çš„å¸‚åœºé£é™©å’Œè´¢åŠ¡é£é™©ã€‚",
                "è¯„ä¼°ä¸€ä¸ªITé¡¹ç›®çš„æŠ€æœ¯é£é™©å’Œè¿›åº¦é£é™©ã€‚"
            ],
            "medium": [
                "è¯„ä¼°ä¸€ä¸ªè·¨å›½ä¼ä¸šçš„è¿è¥é£é™©ï¼ŒåŒ…æ‹¬æ”¿æ²»é£é™©ã€æ±‡ç‡é£é™©ã€ä¾›åº”é“¾é£é™©å’Œåˆè§„é£é™©ã€‚",
                "è¯„ä¼°ä¸€ä¸ªé‡‘èç§‘æŠ€å…¬å¸çš„ç³»ç»Ÿæ€§é£é™©ï¼ŒåŒ…æ‹¬æŠ€æœ¯é£é™©ã€å¸‚åœºé£é™©ã€æ“ä½œé£é™©å’Œç›‘ç®¡é£é™©ã€‚"
            ],
            "complex": [
                "è¯„ä¼°ä¸€ä¸ªå…¨çƒä¾›åº”é“¾ç½‘ç»œçš„ç»¼åˆé£é™©ï¼ŒåŒ…æ‹¬åœ°ç¼˜æ”¿æ²»ã€è‡ªç„¶ç¾å®³ã€ç½‘ç»œå®‰å…¨ã€æ°”å€™å˜åŒ–å’Œå®è§‚ç»æµæ³¢åŠ¨ç­‰å¤šç»´åº¦é£é™©ï¼Œå»ºç«‹åŠ¨æ€é£é™©è¯„ä¼°å’Œç¼“è§£ä½“ç³»ã€‚",
                "è¯„ä¼°ä¸€ä¸ªæ™ºèƒ½åŸå¸‚ç³»ç»Ÿçš„ç½‘ç»œå®‰å…¨é£é™©ï¼ŒåŒ…æ‹¬å…³é”®åŸºç¡€è®¾æ–½ä¿æŠ¤ã€æ•°æ®éšç§ã€AIç³»ç»Ÿå®‰å…¨å’Œåº”æ€¥å“åº”èƒ½åŠ›ã€‚"
            ]
        }
        
        # æˆ˜ç•¥è§„åˆ’åœºæ™¯
        tasks["strategic_planning"] = {
            "simple": [
                "åˆ¶å®šä¸€ä¸ªåˆåˆ›å…¬å¸çš„3å¹´å‘å±•æˆ˜ç•¥è§„åˆ’ã€‚",
                "åˆ¶å®šä¸€ä¸ªä¼ ç»Ÿä¼ä¸šçš„æ•°å­—åŒ–è½¬å‹æˆ˜ç•¥ã€‚"
            ],
            "medium": [
                "åˆ¶å®šä¸€ä¸ªä¸­å‹ä¼ä¸šçš„å›½é™…åŒ–æ‰©å¼ æˆ˜ç•¥ï¼ŒåŒ…æ‹¬å¸‚åœºé€‰æ‹©ã€è¿›å…¥æ¨¡å¼ã€èµ„æºé…ç½®å’Œé£é™©ç®¡æ§ã€‚",
                "åˆ¶å®šä¸€ä¸ªç§‘æŠ€å…¬å¸çš„äº§å“åˆ›æ–°æˆ˜ç•¥ï¼ŒåŒ…æ‹¬æŠ€æœ¯è·¯çº¿å›¾ã€å¸‚åœºå®šä½ã€ç«äº‰ç­–ç•¥å’Œå•†ä¸šæ¨¡å¼ã€‚"
            ],
            "complex": [
                "åˆ¶å®šä¸€ä¸ªå¤§å‹é›†å›¢çš„å¤šå…ƒåŒ–å‘å±•æˆ˜ç•¥ï¼ŒåŒ…æ‹¬äº§ä¸šå¸ƒå±€ã€èµ„æºé…ç½®ã€ç»„ç»‡å˜é©ã€æ–‡åŒ–èåˆå’Œå¯æŒç»­å‘å±•ï¼Œå»ºç«‹åŠ¨æ€æˆ˜ç•¥è°ƒæ•´æœºåˆ¶ã€‚",
                "åˆ¶å®šä¸€ä¸ªå›½å®¶çš„æ•°å­—ç»æµå‘å±•æˆ˜ç•¥ï¼ŒåŒ…æ‹¬åŸºç¡€è®¾æ–½å»ºè®¾ã€äº§ä¸šå‡çº§ã€äººæ‰åŸ¹å…»ã€æ”¿ç­–æ³•è§„å’Œå›½é™…åˆä½œï¼Œæ„å»ºå®Œæ•´çš„æ•°å­—ç»æµç”Ÿæ€ç³»ç»Ÿã€‚"
            ]
        }
        
        return tasks
    
    @contextmanager
    def memory_tracking(self, framework: str, scenario: str, agent_count: int, complexity: str = "medium"):
        """å†…å­˜è·Ÿè¸ªä¸Šä¸‹æ–‡ç®¡ç†å™¨ - ä¿®å¤ç‰ˆæœ¬"""
        class MemoryTracker:
            def __init__(self, parent, framework, scenario, agent_count, complexity):
                self.parent = parent
                self.framework = framework
                self.scenario = scenario
                self.agent_count = agent_count
                self.complexity = complexity
                self.initial_memory = None
                self.peak_memory = 0
                
            def __enter__(self):
                # å¼ºåˆ¶åƒåœ¾å›æ”¶
                gc.collect()
                process = psutil.Process()
                self.initial_memory = process.memory_info().rss / 1024 / 1024  # MB
                self.peak_memory = self.initial_memory
                self.monitoring_active = True
                
                # å¯åŠ¨å†…å­˜ç›‘æ§çº¿ç¨‹
                self.monitor_thread = threading.Thread(target=self._monitor_memory)
                self.monitor_thread.daemon = True
                self.monitor_thread.start()
                
                logger.info(f"åˆå§‹å†…å­˜: {self.initial_memory:.2f} MB")
                return self
                
            def __exit__(self, exc_type, exc_val, exc_tb):
                # åœæ­¢å†…å­˜ç›‘æ§
                self.monitoring_active = False
                if hasattr(self, 'monitor_thread'):
                    self.monitor_thread.join(timeout=1.0)
                
                process = psutil.Process()
                final_memory = process.memory_info().rss / 1024 / 1024  # MB
                
                # è®¡ç®—å†…å­˜ä½¿ç”¨ - ä½¿ç”¨å³°å€¼å†…å­˜è€Œä¸æ˜¯æœ€ç»ˆå†…å­˜
                memory_usage = max(0, self.peak_memory - self.initial_memory)
                
                # ç¡®ä¿é”®æ ¼å¼ä¸€è‡´
                key = f"{self.framework}_{self.scenario}_{self.agent_count}_{self.complexity}"
                self.parent.memory_tracker[key] = memory_usage
                
                logger.info(f"å†…å­˜ä½¿ç”¨è®°å½•: {key}")
                logger.info(f"  åˆå§‹å†…å­˜: {self.initial_memory:.2f} MB")
                logger.info(f"  å³°å€¼å†…å­˜: {self.peak_memory:.2f} MB") 
                logger.info(f"  æœ€ç»ˆå†…å­˜: {final_memory:.2f} MB")
                logger.info(f"  å†…å­˜ä½¿ç”¨: {memory_usage:.2f} MB")
            
            def _monitor_memory(self):
                """å†…å­˜ç›‘æ§çº¿ç¨‹"""
                process = psutil.Process()
                while self.monitoring_active:
                    try:
                        current_memory = process.memory_info().rss / 1024 / 1024  # MB
                        if current_memory > self.peak_memory:
                            self.peak_memory = current_memory
                        time.sleep(0.1)  # æ¯100msæ£€æŸ¥ä¸€æ¬¡
                    except:
                        break
        
        tracker = MemoryTracker(self, framework, scenario, agent_count, complexity)
        try:
            yield tracker
        finally:
            pass
    
    def test_our_dsl_real_api(self, scenario: str, agent_count: int, complexity: str = "medium") -> Dict[str, Any]:
        """æµ‹è¯•Our DSLæ¡†æ¶ï¼ˆçœŸå®APIè°ƒç”¨ï¼‰"""
        try:
            from dsl.dsl import DSL
            from core.robust_llm import llm_callable
            
            tasks = self.tasks[scenario][complexity]
            
            # æ”¹è¿›çš„å†…å­˜ç›‘æ§
            gc.collect()
            process = psutil.Process()
            initial_memory = process.memory_info().rss / 1024 / 1024  # MB
            peak_memory = initial_memory
            
            start_time = time.time()
            
            # åˆ›å»ºDSLå®ä¾‹
            dsl = DSL(seed=self.random_seed, workers=min(agent_count, 8))
            
            # æ·»åŠ ä»»åŠ¡
            task_objects = []
            for i, task_prompt in enumerate(tasks[:agent_count]):
                task_obj = dsl.gen(f"task_{i}", prompt=task_prompt, agent=f"agent_{i}").schedule()
                task_objects.append(task_obj)
            
            # è¿è¡ŒDSL
            dsl.run(llm_callable)
            
            # ç­‰å¾…ä»»åŠ¡å®Œæˆ
            successful_tasks = 0
            for task in task_objects:
                try:
                    result = task.wait(timeout=120.0)  # å¢åŠ è¶…æ—¶æ—¶é—´
                    if result is not None and len(str(result).strip()) > 100:  # æé«˜è´¨é‡è¦æ±‚
                        successful_tasks += 1
                    
                    # åœ¨ä»»åŠ¡æ‰§è¡Œè¿‡ç¨‹ä¸­ç›‘æ§å†…å­˜
                    current_memory = process.memory_info().rss / 1024 / 1024
                    if current_memory > peak_memory:
                        peak_memory = current_memory
                        
                except Exception as e:
                    logger.warning(f"ä»»åŠ¡ç­‰å¾…å¤±è´¥: {e}")
            
            execution_time = time.time() - start_time
            throughput = successful_tasks / execution_time if execution_time > 0 else 0
            avg_latency = execution_time / successful_tasks if successful_tasks > 0 else 0
            success_rate = (successful_tasks / len(tasks[:agent_count])) * 100
            
            # è®¡ç®—å†…å­˜ä½¿ç”¨
            memory_usage = max(0, peak_memory - initial_memory)
            
            logger.info(f"Our DSLå†…å­˜ä½¿ç”¨: {memory_usage:.2f} MB (åˆå§‹: {initial_memory:.2f}, å³°å€¼: {peak_memory:.2f})")
            
            return {
                "framework": "Our DSL",
                "scenario": scenario,
                "agent_count": agent_count,
                "complexity": complexity,
                "execution_time": execution_time,
                "throughput": throughput,
                "success_rate": success_rate,
                "successful_tasks": successful_tasks,
                "total_tasks": len(tasks[:agent_count]),
                "avg_latency": avg_latency * 1000,  # è½¬æ¢ä¸ºms
                "status": "success",
                "memory_usage": memory_usage,
                "api_type": "real_api"
            }
                
        except Exception as e:
            logger.error(f"Our DSLæµ‹è¯•å¤±è´¥: {e}")
            return {
                "framework": "Our DSL",
                "scenario": scenario,
                "agent_count": agent_count,
                "complexity": complexity,
                "execution_time": 0,
                "throughput": 0,
                "success_rate": 0,
                "successful_tasks": 0,
                "total_tasks": len(self.tasks[scenario][complexity][:agent_count]),
                "avg_latency": 0,
                "status": "error",
                "memory_usage": 0,
                "error": str(e),
                "api_type": "error"
            }
    
    def test_langchain_real_api(self, scenario: str, agent_count: int, complexity: str = "medium") -> Dict[str, Any]:
        """æµ‹è¯•LangChainæ¡†æ¶ï¼ˆçœŸå®APIè°ƒç”¨ï¼‰"""
        try:
            from langchain_openai import ChatOpenAI
            from langchain.schema import HumanMessage
            
            tasks = self.tasks[scenario][complexity]
            
            with self.memory_tracking("langchain", scenario, agent_count, complexity):
                start_time = time.time()
                
                # åˆ›å»ºLangChainå®¢æˆ·ç«¯
                llm = ChatOpenAI(
                    model="gpt-4o-mini",
                    api_key=self.api_key,
                    base_url=self.base_url,
                    temperature=0.3,
                    max_tokens=1000  # å¢åŠ tokenæ•°é‡
                )
                
                # æ‰§è¡Œä»»åŠ¡
                successful_tasks = 0
                for i, task_prompt in enumerate(tasks[:agent_count]):
                    try:
                        message = HumanMessage(content=task_prompt)
                        response = llm.invoke([message])
                        if response and response.content and len(response.content.strip()) > 100:
                            successful_tasks += 1
                    except Exception as e:
                        logger.warning(f"LangChainä»»åŠ¡{i}å¤±è´¥: {e}")
                
                execution_time = time.time() - start_time
                throughput = successful_tasks / execution_time if execution_time > 0 else 0
                avg_latency = execution_time / successful_tasks if successful_tasks > 0 else 0
                success_rate = (successful_tasks / len(tasks[:agent_count])) * 100
                
            # åœ¨withè¯­å¥ç»“æŸåè·å–å†…å­˜ä½¿ç”¨
            memory_usage = self.memory_tracker.get(f"langchain_{scenario}_{agent_count}_{complexity}", 0)
            
            return {
                "framework": "LangChain",
                "scenario": scenario,
                "agent_count": agent_count,
                "complexity": complexity,
                "execution_time": execution_time,
                "throughput": throughput,
                "success_rate": success_rate,
                "successful_tasks": successful_tasks,
                "total_tasks": len(tasks[:agent_count]),
                "avg_latency": avg_latency * 1000,  # è½¬æ¢ä¸ºms
                "status": "success",
                "memory_usage": memory_usage,
                "api_type": "real_api"
            }
            
        except Exception as e:
            logger.error(f"LangChainæµ‹è¯•å¤±è´¥: {e}")
            return {
                "framework": "LangChain",
                "scenario": scenario,
                "agent_count": agent_count,
                "complexity": complexity,
                "execution_time": 0,
                "throughput": 0,
                "success_rate": 0,
                "successful_tasks": 0,
                "total_tasks": len(self.tasks[scenario][complexity][:agent_count]),
                "avg_latency": 0,
                "status": "error",
                "memory_usage": 0,
                "error": str(e),
                "api_type": "error"
            }
    
    def test_crewai_real_api(self, scenario: str, agent_count: int, complexity: str = "medium") -> Dict[str, Any]:
        """æµ‹è¯•CrewAIæ¡†æ¶ï¼ˆçœŸå®APIè°ƒç”¨ï¼‰"""
        try:
            from crewai import Agent, Task, Crew, Process
            from crewai.llm import LLM
            
            tasks = self.tasks[scenario][complexity]
            
            with self.memory_tracking("crewai", scenario, agent_count, complexity):
                start_time = time.time()
                
                # åˆ›å»ºCrewAI LLM
                llm = LLM(
                    model="gpt-4o-mini",
                    api_key=self.api_key,
                    base_url=self.base_url,
                    temperature=0.3,
                    max_tokens=1000
                )
                
                # åˆ›å»ºä»£ç†
                agents = []
                for i in range(min(agent_count, 3)):  # CrewAIé™åˆ¶ä»£ç†æ•°é‡
                    agent = Agent(
                        role=f"Specialist_{i}",
                        goal=f"Complete {scenario} tasks efficiently and accurately",
                        backstory=f"You are an expert in {scenario} with deep domain knowledge and analytical skills.",
                        llm=llm,
                        verbose=False,
                        allow_delegation=False
                    )
                    agents.append(agent)
                
                # åˆ›å»ºä»»åŠ¡
                crew_tasks = []
                for i, task_prompt in enumerate(tasks[:agent_count]):
                    task = Task(
                        description=task_prompt,
                        agent=agents[i % len(agents)],
                        expected_output="A detailed and comprehensive response with actionable insights",
                        async_execution=False
                    )
                    crew_tasks.append(task)
                
                # åˆ›å»ºCrewå¹¶æ‰§è¡Œ
                crew = Crew(
                    agents=agents,
                    tasks=crew_tasks,
                    verbose=False,
                    process=Process.sequential
                )
                
                result = crew.kickoff()
                
                # è®¡ç®—æˆåŠŸä»»åŠ¡æ•°
                successful_tasks = 0
                if result:
                    result_str = str(result).strip()
                    if len(result_str) > 100:
                        # æ£€æŸ¥æ˜¯å¦åŒ…å«å¤šä¸ªä»»åŠ¡çš„å“åº”
                        task_responses = result_str.split('\n\n')
                        successful_tasks = min(len(task_responses), len(tasks[:agent_count]))
                    else:
                        successful_tasks = 1 if len(result_str) > 100 else 0
                else:
                    successful_tasks = 0
                
                execution_time = time.time() - start_time
                throughput = successful_tasks / execution_time if execution_time > 0 else 0
                avg_latency = execution_time / successful_tasks if successful_tasks > 0 else 0
                success_rate = (successful_tasks / len(tasks[:agent_count])) * 100
                
            # åœ¨withè¯­å¥ç»“æŸåè·å–å†…å­˜ä½¿ç”¨
            memory_usage = self.memory_tracker.get(f"crewai_{scenario}_{agent_count}_{complexity}", 0)
            
            return {
                "framework": "CrewAI",
                "scenario": scenario,
                "agent_count": agent_count,
                "complexity": complexity,
                "execution_time": execution_time,
                "throughput": throughput,
                "success_rate": success_rate,
                "successful_tasks": successful_tasks,
                "total_tasks": len(tasks[:agent_count]),
                "avg_latency": avg_latency * 1000,  # è½¬æ¢ä¸ºms
                "status": "success",
                "memory_usage": memory_usage,
                "api_type": "real_api"
            }
            
        except Exception as e:
            logger.error(f"CrewAIæµ‹è¯•å¤±è´¥: {e}")
            return {
                "framework": "CrewAI",
                "scenario": scenario,
                "agent_count": agent_count,
                "complexity": complexity,
                "execution_time": 0,
                "throughput": 0,
                "success_rate": 0,
                "successful_tasks": 0,
                "total_tasks": len(self.tasks[scenario][complexity][:agent_count]),
                "avg_latency": 0,
                "status": "error",
                "memory_usage": 0,
                "error": str(e),
                "api_type": "error"
            }
    
    def test_autogen_real_api(self, scenario: str, agent_count: int, complexity: str = "medium") -> Dict[str, Any]:
        """æµ‹è¯•AutoGenæ¡†æ¶ï¼ˆçœŸå®APIè°ƒç”¨ï¼‰"""
        try:
            from autogen import ConversableAgent, GroupChat, GroupChatManager
            
            tasks = self.tasks[scenario][complexity]
            
            with self.memory_tracking("autogen", scenario, agent_count, complexity):
                start_time = time.time()
                
                # é…ç½®LLM
                llm_config = {
                    "model": "gpt-4o-mini",
                    "api_key": self.api_key,
                    "base_url": self.base_url,
                    "temperature": 0.3,
                    "max_tokens": 1000
                }
                
                # åˆ›å»ºä»£ç†
                agents = []
                for i in range(max(agent_count, 2)):  # AutoGenéœ€è¦è‡³å°‘2ä¸ªä»£ç†
                    agent = ConversableAgent(
                        name=f"agent_{i}",
                        llm_config=llm_config,
                        system_message=f"You are an expert in {scenario} with specialized knowledge and analytical capabilities."
                    )
                    agents.append(agent)
                
                # åˆ›å»ºç¾¤èŠ
                group_chat = GroupChat(
                    agents=agents,
                    messages=[],
                    max_round=5,  # å¢åŠ è½®æ•°
                    speaker_selection_method='round_robin'
                )
                
                manager = GroupChatManager(
                    groupchat=group_chat,
                    llm_config=llm_config
                )
                
                # æ‰§è¡Œä»»åŠ¡
                successful_tasks = 0
                for i, task_prompt in enumerate(tasks[:agent_count]):
                    try:
                        result = agents[0].initiate_chat(
                            manager,
                            message=task_prompt,
                            max_turns=3  # å¢åŠ è½®æ•°
                        )
                        if result and len(str(result).strip()) > 100:
                            successful_tasks += 1
                    except Exception as e:
                        logger.warning(f"AutoGenä»»åŠ¡{i}å¤±è´¥: {e}")
                
                execution_time = time.time() - start_time
                throughput = successful_tasks / execution_time if execution_time > 0 else 0
                avg_latency = execution_time / successful_tasks if successful_tasks > 0 else 0
                success_rate = (successful_tasks / len(tasks[:agent_count])) * 100
                
            # åœ¨withè¯­å¥ç»“æŸåè·å–å†…å­˜ä½¿ç”¨
            memory_usage = self.memory_tracker.get(f"autogen_{scenario}_{agent_count}_{complexity}", 0)
            
            return {
                "framework": "AutoGen",
                "scenario": scenario,
                "agent_count": agent_count,
                "complexity": complexity,
                "execution_time": execution_time,
                "throughput": throughput,
                "success_rate": success_rate,
                "successful_tasks": successful_tasks,
                "total_tasks": len(tasks[:agent_count]),
                "avg_latency": avg_latency * 1000,  # è½¬æ¢ä¸ºms
                "status": "success",
                "memory_usage": memory_usage,
                "api_type": "real_api"
            }
            
        except Exception as e:
            logger.error(f"AutoGenæµ‹è¯•å¤±è´¥: {e}")
            return {
                "framework": "AutoGen",
                "scenario": scenario,
                "agent_count": agent_count,
                "complexity": complexity,
                "execution_time": 0,
                "throughput": 0,
                "success_rate": 0,
                "successful_tasks": 0,
                "total_tasks": len(self.tasks[scenario][complexity][:agent_count]),
                "avg_latency": 0,
                "status": "error",
                "memory_usage": 0,
                "error": str(e),
                "api_type": "error"
            }
    
    def run_comprehensive_benchmark(self) -> Dict[str, Any]:
        """è¿è¡Œå…¨é¢åŸºå‡†æµ‹è¯•"""
        logger.info("ğŸš€ å¼€å§‹å…¨é¢åŸºå‡†æµ‹è¯•...")
        logger.info(f"å®éªŒé…ç½®: {len(self.experiment_config['scenarios'])}ä¸ªåœºæ™¯, "
                   f"{len(self.experiment_config['agent_counts'])}ä¸ªä»£ç†æ•°é‡, "
                   f"{len(self.experiment_config['frameworks'])}ä¸ªæ¡†æ¶, "
                   f"{len(self.experiment_config['task_complexities'])}ä¸ªå¤æ‚åº¦")
        
        benchmark_results = []
        total_tests = (len(self.experiment_config['scenarios']) * 
                      len(self.experiment_config['agent_counts']) * 
                      len(self.experiment_config['frameworks']) * 
                      len(self.experiment_config['task_complexities']) * 
                      self.experiment_config['repetitions'])
        
        current_test = 0
        
        for scenario in self.experiment_config['scenarios']:
            for agent_count in self.experiment_config['agent_counts']:
                for framework in self.experiment_config['frameworks']:
                    for complexity in self.experiment_config['task_complexities']:
                        for repetition in range(self.experiment_config['repetitions']):
                            current_test += 1
                            logger.info(f"ğŸ“Š æµ‹è¯•è¿›åº¦: {current_test}/{total_tests} - "
                                       f"{framework} - {scenario} - {agent_count} agents - {complexity}")
                            
                            # è¿è¡Œæµ‹è¯•
                            if framework == "Our DSL":
                                result = self.test_our_dsl_real_api(scenario, agent_count, complexity)
                            elif framework == "LangChain":
                                result = self.test_langchain_real_api(scenario, agent_count, complexity)
                            elif framework == "CrewAI":
                                result = self.test_crewai_real_api(scenario, agent_count, complexity)
                            elif framework == "AutoGen":
                                result = self.test_autogen_real_api(scenario, agent_count, complexity)
                            
                            # æ·»åŠ é‡å¤æ¬¡æ•°ä¿¡æ¯
                            result["repetition"] = repetition + 1
                            benchmark_results.append(result)
                            
                            # è®°å½•ç»“æœ
                            if result["status"] == "success":
                                logger.info(f"   âœ… æˆåŠŸ: ååé‡={result['throughput']:.3f} tasks/sec, "
                                           f"å»¶è¿Ÿ={result['avg_latency']:.2f} ms, "
                                           f"å†…å­˜={result['memory_usage']:.2f} MB")
                            else:
                                logger.error(f"   âŒ å¤±è´¥: {result.get('error', 'Unknown error')}")
                            
                            # çŸ­æš‚ä¼‘æ¯ä»¥é¿å…APIé™åˆ¶
                            time.sleep(1)
        
        # ä¿å­˜ç»“æœ
        results_file = "comprehensive_benchmark_results.json"
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump({
                "benchmark_results": benchmark_results,
                "memory_tracker": self.memory_tracker,
                "timestamp": datetime.now().isoformat(),
                "experiment_config": self.experiment_config,
                "test_info": {
                    "total_tests": len(benchmark_results),
                    "scenarios": self.experiment_config['scenarios'],
                    "agent_counts": self.experiment_config['agent_counts'],
                    "frameworks": self.experiment_config['frameworks'],
                    "complexities": self.experiment_config['task_complexities'],
                    "repetitions": self.experiment_config['repetitions'],
                    "random_seed": self.random_seed
                }
            }, f, indent=2, ensure_ascii=False)
        
        logger.info(f"âœ… æµ‹è¯•å®Œæˆï¼ç»“æœå·²ä¿å­˜åˆ°: {results_file}")
        
        # ç”Ÿæˆç»Ÿè®¡æ‘˜è¦
        self._generate_summary(benchmark_results)
        
        return {
            "benchmark_results": benchmark_results,
            "memory_tracker": self.memory_tracker,
            "timestamp": datetime.now().isoformat(),
            "experiment_config": self.experiment_config
        }
    
    def _generate_summary(self, benchmark_results: List[Dict[str, Any]]):
        """ç”Ÿæˆç»Ÿè®¡æ‘˜è¦"""
        logger.info("\nğŸ“Š æµ‹è¯•æ‘˜è¦:")
        logger.info("=" * 50)
        
        # æŒ‰æ¡†æ¶åˆ†ç»„ç»Ÿè®¡
        framework_stats = {}
        for result in benchmark_results:
            if result["status"] == "success":
                framework = result["framework"]
                if framework not in framework_stats:
                    framework_stats[framework] = {
                        "throughput": [],
                        "latency": [],
                        "memory": [],
                        "success_rate": [],
                        "total_tests": 0,
                        "successful_tests": 0
                    }
                
                framework_stats[framework]["throughput"].append(result["throughput"])
                framework_stats[framework]["latency"].append(result["avg_latency"])
                framework_stats[framework]["memory"].append(result["memory_usage"])
                framework_stats[framework]["success_rate"].append(result["success_rate"])
                framework_stats[framework]["total_tests"] += 1
                if result["success_rate"] > 0:
                    framework_stats[framework]["successful_tests"] += 1
        
        # è¾“å‡ºç»Ÿè®¡ç»“æœ
        for framework, stats in framework_stats.items():
            if stats["throughput"]:
                avg_throughput = np.mean(stats["throughput"])
                avg_latency = np.mean(stats["latency"])
                avg_memory = np.mean(stats["memory"])
                avg_success_rate = np.mean(stats["success_rate"])
                overall_success_rate = (stats["successful_tests"] / stats["total_tests"]) * 100
                
                logger.info(f"\n{framework}:")
                logger.info(f"  å¹³å‡ååé‡: {avg_throughput:.3f} tasks/sec")
                logger.info(f"  å¹³å‡å»¶è¿Ÿ: {avg_latency:.2f} ms")
                logger.info(f"  å¹³å‡å†…å­˜ä½¿ç”¨: {avg_memory:.2f} MB")
                logger.info(f"  å¹³å‡æˆåŠŸç‡: {avg_success_rate:.1f}%")
                logger.info(f"  æ€»ä½“æˆåŠŸç‡: {overall_success_rate:.1f}%")
                logger.info(f"  æˆåŠŸæµ‹è¯•æ•°: {stats['successful_tests']}/{stats['total_tests']}")
        
        # æ€»ä½“ç»Ÿè®¡
        total_tests = len(benchmark_results)
        successful_tests = sum(1 for r in benchmark_results if r["status"] == "success")
        overall_success_rate = (successful_tests / total_tests) * 100
        
        logger.info(f"\nğŸ“ˆ æ€»ä½“ç»Ÿè®¡:")
        logger.info(f"  æˆåŠŸæµ‹è¯•: {successful_tests}")
        logger.info(f"  å¤±è´¥æµ‹è¯•: {total_tests - successful_tests}")
        logger.info(f"  æˆåŠŸç‡: {overall_success_rate:.1f}%")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ”¬ å…¨é¢åŸºå‡†æµ‹è¯•æ¡†æ¶")
    print("=" * 50)
    print("âš ï¸  æ³¨æ„ï¼šæ­¤æµ‹è¯•ä½¿ç”¨çœŸå®APIè°ƒç”¨ï¼Œéœ€è¦æœ‰æ•ˆçš„APIå¯†é’¥")
    print("âš ï¸  æµ‹è¯•è§„æ¨¡è¾ƒå¤§ï¼Œé¢„è®¡éœ€è¦è¾ƒé•¿æ—¶é—´å®Œæˆ")
    print("=" * 50)
    
    # åˆ›å»ºåŸºå‡†æµ‹è¯•å®ä¾‹
    benchmark = ComprehensiveBenchmark()
    
    # è¿è¡Œå…¨é¢åŸºå‡†æµ‹è¯•
    results = benchmark.run_comprehensive_benchmark()
    
    print("\nğŸ‰ å…¨é¢åŸºå‡†æµ‹è¯•å®Œæˆï¼")
    print(f"æ€»æµ‹è¯•æ•°: {len(results['benchmark_results'])}")
    print("ç»“æœå·²ä¿å­˜åˆ° comprehensive_benchmark_results.json")

if __name__ == "__main__":
    main()
