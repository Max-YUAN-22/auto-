#!/usr/bin/env python3
"""
æ€§èƒ½å¯¹æ¯”åˆ†æ - å±•ç¤ºçœŸå®æ€§èƒ½æå‡
Performance Comparison Analysis - Show Real Performance Improvements
"""

import json
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
from typing import Dict, List, Any

def load_benchmark_results(file_path: str) -> Dict[str, Any]:
    """åŠ è½½åŸºå‡†æµ‹è¯•ç»“æœ"""
    with open(file_path, 'r', encoding='utf-8') as f:
        return json.load(f)

def analyze_performance(results: Dict[str, Any]) -> Dict[str, Any]:
    """åˆ†ææ€§èƒ½æ•°æ®"""
    framework_stats = {}
    
    for result in results["benchmark_results"]:
        framework = result["framework"]
        if framework not in framework_stats:
            framework_stats[framework] = {
                "throughputs": [],
                "latencies": [],
                "memory_usage": [],
                "success_rates": [],
                "execution_times": []
            }
        
        if result["status"] == "success":
            framework_stats[framework]["throughputs"].append(result["throughput"])
            framework_stats[framework]["latencies"].append(result["avg_latency"])
            framework_stats[framework]["memory_usage"].append(result["memory_usage"])
            framework_stats[framework]["success_rates"].append(result["success_rate"])
            framework_stats[framework]["execution_times"].append(result["execution_time"])
    
    # è®¡ç®—ç»Ÿè®¡æŒ‡æ ‡
    analysis = {}
    for framework, stats in framework_stats.items():
        analysis[framework] = {
            "avg_throughput": np.mean(stats["throughputs"]),
            "max_throughput": np.max(stats["throughputs"]),
            "min_throughput": np.min(stats["throughputs"]),
            "std_throughput": np.std(stats["throughputs"]),
            "avg_latency": np.mean(stats["latencies"]),
            "avg_memory": np.mean(stats["memory_usage"]),
            "avg_success_rate": np.mean(stats["success_rates"]),
            "total_tests": len(stats["throughputs"])
        }
    
    return analysis

def compare_performance(old_results: Dict[str, Any], new_results: Dict[str, Any]) -> Dict[str, Any]:
    """å¯¹æ¯”æ–°æ—§æ€§èƒ½"""
    old_analysis = analyze_performance(old_results)
    new_analysis = analyze_performance(new_results)
    
    comparison = {}
    
    # å¯¹æ¯”Our DSLçš„æ€§èƒ½æå‡
    if "Our DSL" in old_analysis and "Our Fast DSL" in new_analysis:
        old_dsl = old_analysis["Our DSL"]
        new_dsl = new_analysis["Our Fast DSL"]
        
        comparison["Our DSL Performance Improvement"] = {
            "throughput_improvement": (new_dsl["avg_throughput"] - old_dsl["avg_throughput"]) / old_dsl["avg_throughput"] * 100,
            "latency_reduction": (old_dsl["avg_latency"] - new_dsl["avg_latency"]) / old_dsl["avg_latency"] * 100,
            "memory_efficiency": (old_dsl["avg_memory"] - new_dsl["avg_memory"]) / old_dsl["avg_memory"] * 100,
            "old_throughput": old_dsl["avg_throughput"],
            "new_throughput": new_dsl["avg_throughput"],
            "old_latency": old_dsl["avg_latency"],
            "new_latency": new_dsl["avg_latency"]
        }
    
    # å¯¹æ¯”æ‰€æœ‰æ¡†æ¶
    comparison["Framework Rankings"] = {}
    
    # ååé‡æ’å
    throughput_ranking = sorted(new_analysis.items(), key=lambda x: x[1]["avg_throughput"], reverse=True)
    comparison["Framework Rankings"]["Throughput"] = [(name, stats["avg_throughput"]) for name, stats in throughput_ranking]
    
    # å»¶è¿Ÿæ’å
    latency_ranking = sorted(new_analysis.items(), key=lambda x: x[1]["avg_latency"])
    comparison["Framework Rankings"]["Latency"] = [(name, stats["avg_latency"]) for name, stats in latency_ranking]
    
    # å†…å­˜æ•ˆç‡æ’å
    memory_ranking = sorted(new_analysis.items(), key=lambda x: x[1]["avg_memory"])
    comparison["Framework Rankings"]["Memory Efficiency"] = [(name, stats["avg_memory"]) for name, stats in memory_ranking]
    
    return comparison

def print_performance_report(comparison: Dict[str, Any]):
    """æ‰“å°æ€§èƒ½æŠ¥å‘Š"""
    print("=" * 80)
    print("ğŸš€ é«˜æ€§èƒ½DSLåŸºå‡†æµ‹è¯•ç»“æœåˆ†æ")
    print("=" * 80)
    
    if "Our DSL Performance Improvement" in comparison:
        improvement = comparison["Our DSL Performance Improvement"]
        print(f"\nğŸ“ˆ Our DSLæ€§èƒ½æå‡:")
        print(f"   ååé‡æå‡: {improvement['throughput_improvement']:.1f}%")
        print(f"   å»¶è¿Ÿå‡å°‘: {improvement['latency_reduction']:.1f}%")
        print(f"   å†…å­˜æ•ˆç‡æå‡: {improvement['memory_efficiency']:.1f}%")
        print(f"   æ—§ååé‡: {improvement['old_throughput']:.2f} tasks/sec")
        print(f"   æ–°ååé‡: {improvement['new_throughput']:.2f} tasks/sec")
        print(f"   æ—§å»¶è¿Ÿ: {improvement['old_latency']*1000:.3f} ms")
        print(f"   æ–°å»¶è¿Ÿ: {improvement['new_latency']*1000:.3f} ms")
    
    print(f"\nğŸ† æ¡†æ¶æ€§èƒ½æ’å:")
    
    if "Framework Rankings" in comparison:
        rankings = comparison["Framework Rankings"]
        
        print(f"\n   ååé‡æ’å (tasks/sec):")
        for i, (name, throughput) in enumerate(rankings["Throughput"], 1):
            print(f"     {i}. {name}: {throughput:.2f}")
        
        print(f"\n   å»¶è¿Ÿæ’å (ç§’):")
        for i, (name, latency) in enumerate(rankings["Latency"], 1):
            print(f"     {i}. {name}: {latency*1000:.3f} ms")
        
        print(f"\n   å†…å­˜æ•ˆç‡æ’å (MB):")
        for i, (name, memory) in enumerate(rankings["Memory Efficiency"], 1):
            print(f"     {i}. {name}: {memory:.2f}")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ“Š åŠ è½½åŸºå‡†æµ‹è¯•ç»“æœ...")
    
    # åŠ è½½æ—§ç‰ˆæœ¬ç»“æœ
    try:
        old_results = load_benchmark_results("results/perfectly_fair_benchmark_results.json")
        print("âœ… å·²åŠ è½½åŸå§‹DSLåŸºå‡†æµ‹è¯•ç»“æœ")
    except FileNotFoundError:
        print("âŒ æœªæ‰¾åˆ°åŸå§‹DSLåŸºå‡†æµ‹è¯•ç»“æœ")
        old_results = None
    
    # åŠ è½½æ–°ç‰ˆæœ¬ç»“æœ
    try:
        new_results = load_benchmark_results("results/high_performance_benchmark_results.json")
        print("âœ… å·²åŠ è½½é«˜æ€§èƒ½DSLåŸºå‡†æµ‹è¯•ç»“æœ")
    except FileNotFoundError:
        print("âŒ æœªæ‰¾åˆ°é«˜æ€§èƒ½DSLåŸºå‡†æµ‹è¯•ç»“æœ")
        return
    
    # åˆ†ææ€§èƒ½
    print("\nğŸ” åˆ†ææ€§èƒ½æ•°æ®...")
    new_analysis = analyze_performance(new_results)
    
    # å¯¹æ¯”æ€§èƒ½
    if old_results:
        print("ğŸ“ˆ å¯¹æ¯”æ€§èƒ½æå‡...")
        comparison = compare_performance(old_results, new_results)
        print_performance_report(comparison)
    else:
        print("\nğŸ“Š å½“å‰æ€§èƒ½åˆ†æ:")
        for framework, stats in new_analysis.items():
            print(f"\n   {framework}:")
            print(f"     å¹³å‡ååé‡: {stats['avg_throughput']:.2f} tasks/sec")
            print(f"     å¹³å‡å»¶è¿Ÿ: {stats['avg_latency']*1000:.3f} ms")
            print(f"     å¹³å‡å†…å­˜ä½¿ç”¨: {stats['avg_memory']:.2f} MB")
            print(f"     æˆåŠŸç‡: {stats['avg_success_rate']:.2%}")
    
    print("\n" + "=" * 80)
    print("âœ… æ€§èƒ½åˆ†æå®Œæˆï¼")

if __name__ == "__main__":
    main()
