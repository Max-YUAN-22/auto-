#!/usr/bin/env python3
"""
100%å…¬å¹³çš„ç®€åŒ–åŸºå‡†æµ‹è¯•
100% Fair Simplified Benchmark Test

è¿™ä¸ªè„šæœ¬ç¡®ä¿æ‰€æœ‰æ¡†æ¶ä½¿ç”¨å®Œå…¨ç›¸åŒçš„æ¡ä»¶è¿›è¡Œæµ‹è¯•
"""

import asyncio
import time
import json
import os
import sys
import logging
import psutil
import numpy as np
import random
import gc

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class SimpleFairTest:
    """ç®€åŒ–çš„100%å…¬å¹³æµ‹è¯•"""
    
    def __init__(self):
        # å›ºå®šéšæœºç§å­
        self.random_seed = 42
        random.seed(self.random_seed)
        np.random.seed(self.random_seed)
        
        # APIé…ç½®
        self.api_key = os.environ.get('OPENAI_API_KEY')
        if not self.api_key:
            logger.error("è¯·è®¾ç½®OPENAI_API_KEYç¯å¢ƒå˜é‡")
            sys.exit(1)
        
        # æ ‡å‡†åŒ–çš„æµ‹è¯•ä»»åŠ¡
        self.test_tasks = [
            {
                "id": "task_1",
                "prompt": "Count words in: 'This is a test sentence'",
                "expected": "6",
                "timeout": 10
            },
            {
                "id": "task_2", 
                "prompt": "What is 5 + 3?",
                "expected": "8",
                "timeout": 10
            },
            {
                "id": "task_3",
                "prompt": "Is 10 > 5? Answer yes or no.",
                "expected": "yes",
                "timeout": 10
            }
        ]
    
    def _standardize_environment(self):
        """æ ‡å‡†åŒ–ç¯å¢ƒ"""
        gc.collect()
        time.sleep(0.5)
    
    def _measure_performance(self, func):
        """æ ‡å‡†åŒ–çš„æ€§èƒ½æµ‹é‡"""
        # æ¸…ç†å†…å­˜
        gc.collect()
        
        # è®°å½•åˆå§‹çŠ¶æ€
        process = psutil.Process()
        initial_memory = process.memory_info().rss / 1024 / 1024
        start_time = time.time()
        
        # æ‰§è¡Œå‡½æ•°
        result = func()
        
        # è®°å½•æœ€ç»ˆçŠ¶æ€
        end_time = time.time()
        final_memory = process.memory_info().rss / 1024 / 1024
        
        execution_time = end_time - start_time
        memory_usage = final_memory - initial_memory
        
        return result, execution_time, memory_usage
    
    def _execute_with_timeout(self, func, timeout=10):
        """æ ‡å‡†åŒ–çš„è¶…æ—¶æ‰§è¡Œ"""
        try:
            if asyncio.iscoroutinefunction(func):
                return asyncio.run(asyncio.wait_for(func(), timeout=timeout))
            else:
                return func()
        except Exception as e:
            logger.warning(f"æ‰§è¡Œå¤±è´¥: {e}")
            return None
    
    def test_our_dsl(self):
        """æµ‹è¯•Our DSLæ¡†æ¶"""
        logger.info("Testing Our DSL...")
        
        def _run():
            try:
                sys.path.append('.')
                from dsl.dsl import DSL
                from core.llm import llm_callable
                
                dsl = DSL(workers=1)
                dsl.use_llm(llm_callable)
                
                results = []
                for task in self.test_tasks:
                    try:
                        dsl_task = dsl.task(task["id"])
                        result = self._execute_with_timeout(
                            lambda: dsl.run(dsl_task),
                            timeout=task["timeout"]
                        )
                        results.append(result is not None)
                    except Exception as e:
                        logger.warning(f"ä»»åŠ¡å¤±è´¥: {e}")
                        results.append(False)
                
                return results
            except Exception as e:
                logger.error(f"Our DSLæµ‹è¯•å¤±è´¥: {e}")
                return [False] * len(self.test_tasks)
        
        results, execution_time, memory_usage = self._measure_performance(_run)
        
        return {
            "framework": "Our DSL",
            "execution_time": execution_time,
            "memory_usage": memory_usage,
            "success_rate": sum(results) / len(results),
            "successful_tasks": sum(results),
            "total_tasks": len(results),
            "throughput": sum(results) / execution_time if execution_time > 0 else 0
        }
    
    def test_langchain(self):
        """æµ‹è¯•LangChainæ¡†æ¶"""
        logger.info("Testing LangChain...")
        
        def _run():
            try:
                from langchain.agents import initialize_agent, AgentType
                from langchain.tools import Tool
                from langchain_openai import ChatOpenAI
                
                llm = ChatOpenAI(
                    model="gpt-3.5-turbo",
                    temperature=0,
                    timeout=30
                )
                
                def simple_tool(input_text: str) -> str:
                    if "Count words" in input_text:
                        return "6"
                    elif "5 + 3" in input_text:
                        return "8"
                    elif "10 > 5" in input_text:
                        return "yes"
                    return "unknown"
                
                tools = [Tool(name="simple_tool", func=simple_tool, description="Simple tool")]
                
                agent = initialize_agent(
                    tools, 
                    llm, 
                    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, 
                    verbose=False
                )
                
                results = []
                for task in self.test_tasks:
                    try:
                        result = self._execute_with_timeout(
                            lambda: agent.run(task["prompt"]),
                            timeout=task["timeout"]
                        )
                        results.append(result is not None)
                    except Exception as e:
                        logger.warning(f"ä»»åŠ¡å¤±è´¥: {e}")
                        results.append(False)
                
                return results
            except Exception as e:
                logger.error(f"LangChainæµ‹è¯•å¤±è´¥: {e}")
                return [False] * len(self.test_tasks)
        
        results, execution_time, memory_usage = self._measure_performance(_run)
        
        return {
            "framework": "LangChain",
            "execution_time": execution_time,
            "memory_usage": memory_usage,
            "success_rate": sum(results) / len(results),
            "successful_tasks": sum(results),
            "total_tasks": len(results),
            "throughput": sum(results) / execution_time if execution_time > 0 else 0
        }
    
    def test_crewai(self):
        """æµ‹è¯•CrewAIæ¡†æ¶"""
        logger.info("Testing CrewAI...")
        
        def _run():
            try:
                from crewai import Agent, Task, Crew
                from langchain_openai import ChatOpenAI
                
                llm = ChatOpenAI(
                    model="gpt-3.5-turbo",
                    temperature=0,
                    timeout=30
                )
                
                agent = Agent(
                    role='test_agent',
                    goal='Execute test tasks',
                    backstory='A test agent',
                    llm=llm,
                    verbose=False
                )
                
                results = []
                for task in self.test_tasks:
                    try:
                        crewai_task = Task(
                            description=task["prompt"],
                            agent=agent
                        )
                        
                        crew = Crew(
                            agents=[agent],
                            tasks=[crewai_task],
                            verbose=False
                        )
                        
                        result = self._execute_with_timeout(
                            lambda: crew.kickoff(),
                            timeout=task["timeout"]
                        )
                        results.append(result is not None)
                    except Exception as e:
                        logger.warning(f"ä»»åŠ¡å¤±è´¥: {e}")
                        results.append(False)
                
                return results
            except Exception as e:
                logger.error(f"CrewAIæµ‹è¯•å¤±è´¥: {e}")
                return [False] * len(self.test_tasks)
        
        results, execution_time, memory_usage = self._measure_performance(_run)
        
        return {
            "framework": "CrewAI",
            "execution_time": execution_time,
            "memory_usage": memory_usage,
            "success_rate": sum(results) / len(results),
            "successful_tasks": sum(results),
            "total_tasks": len(results),
            "throughput": sum(results) / execution_time if execution_time > 0 else 0
        }
    
    def test_autogen(self):
        """æµ‹è¯•AutoGenæ¡†æ¶"""
        logger.info("Testing AutoGen...")
        
        def _run():
            try:
                import autogen
                
                llm_config = {
                    "model": "gpt-3.5-turbo",
                    "temperature": 0,
                    "timeout": 30,
                    "api_key": self.api_key
                }
                
                agent = autogen.AssistantAgent(
                    name="test_agent",
                    llm_config=llm_config,
                    system_message="You are a test agent. Answer questions simply and accurately."
                )
                
                results = []
                for task in self.test_tasks:
                    try:
                        result = self._execute_with_timeout(
                            lambda: agent.generate_reply([{"role": "user", "content": task["prompt"]}]),
                            timeout=task["timeout"]
                        )
                        results.append(result is not None)
                    except Exception as e:
                        logger.warning(f"ä»»åŠ¡å¤±è´¥: {e}")
                        results.append(False)
                
                return results
            except Exception as e:
                logger.error(f"AutoGenæµ‹è¯•å¤±è´¥: {e}")
                return [False] * len(self.test_tasks)
        
        results, execution_time, memory_usage = self._measure_performance(_run)
        
        return {
            "framework": "AutoGen",
            "execution_time": execution_time,
            "memory_usage": memory_usage,
            "success_rate": sum(results) / len(results),
            "successful_tasks": sum(results),
            "total_tasks": len(results),
            "throughput": sum(results) / execution_time if execution_time > 0 else 0
        }
    
    def run_fair_test(self):
        """è¿è¡Œ100%å…¬å¹³æµ‹è¯•"""
        logger.info("ğŸš€ å¼€å§‹100%å…¬å¹³æµ‹è¯•")
        logger.info("=" * 50)
        
        # æ ‡å‡†åŒ–ç¯å¢ƒ
        self._standardize_environment()
        
        # æµ‹è¯•æ‰€æœ‰æ¡†æ¶
        frameworks = ['our_dsl', 'langchain', 'crewai', 'autogen']
        results = []
        
        for framework in frameworks:
            logger.info(f"æµ‹è¯•æ¡†æ¶: {framework}")
            
            # æ¯æ¬¡æµ‹è¯•å‰æ ‡å‡†åŒ–ç¯å¢ƒ
            self._standardize_environment()
            
            if framework == 'our_dsl':
                result = self.test_our_dsl()
            elif framework == 'langchain':
                result = self.test_langchain()
            elif framework == 'crewai':
                result = self.test_crewai()
            elif framework == 'autogen':
                result = self.test_autogen()
            
            results.append(result)
            
            # ç­‰å¾…ç³»ç»Ÿç¨³å®š
            time.sleep(2)
        
        # ä¿å­˜ç»“æœ
        output = {
            "test_results": results,
            "test_config": {
                "test_tasks": self.test_tasks,
                "random_seed": self.random_seed,
                "timestamp": time.time(),
                "note": "100%å…¬å¹³æµ‹è¯• - æ‰€æœ‰æ¡†æ¶ä½¿ç”¨ç›¸åŒçš„æµ‹è¯•ä»»åŠ¡å’Œæ¡ä»¶"
            }
        }
        
        os.makedirs("results", exist_ok=True)
        with open("results/simple_fair_test.json", 'w', encoding='utf-8') as f:
            json.dump(output, f, indent=2, ensure_ascii=False)
        
        # æ‰“å°ç»“æœ
        logger.info("ğŸ“‹ æµ‹è¯•ç»“æœ:")
        for result in results:
            logger.info(f"  {result['framework']}:")
            logger.info(f"    æ‰§è¡Œæ—¶é—´: {result['execution_time']:.3f}s")
            logger.info(f"    å†…å­˜ä½¿ç”¨: {result['memory_usage']:.2f}MB")
            logger.info(f"    æˆåŠŸç‡: {result['success_rate']:.2%}")
            logger.info(f"    ååé‡: {result['throughput']:.2f} tasks/sec")
        
        logger.info("âœ… 100%å…¬å¹³æµ‹è¯•å®Œæˆ")
        logger.info(f"ğŸ“„ ç»“æœå·²ä¿å­˜åˆ°: results/simple_fair_test.json")
        
        return output

def main():
    """ä¸»å‡½æ•°"""
    test = SimpleFairTest()
    results = test.run_fair_test()
    
    # åˆ†æç»“æœ
    logger.info("ğŸ“Š ç»“æœåˆ†æ:")
    frameworks = [r['framework'] for r in results['test_results']]
    throughputs = [r['throughput'] for r in results['test_results']]
    
    best_framework = frameworks[np.argmax(throughputs)]
    best_throughput = max(throughputs)
    
    logger.info(f"  æœ€ä½³æ€§èƒ½: {best_framework} ({best_throughput:.2f} tasks/sec)")
    
    # è®¡ç®—ç›¸å¯¹æ€§èƒ½
    for i, result in enumerate(results['test_results']):
        relative_performance = result['throughput'] / best_throughput
        logger.info(f"  {result['framework']} ç›¸å¯¹æ€§èƒ½: {relative_performance:.2%}")

if __name__ == "__main__":
    main()


