#!/usr/bin/env python3
"""
Our DSLæ€§èƒ½ä¼˜åŒ– - çœŸå®æå‡
Our DSL Performance Optimization - Real Improvement
"""

import asyncio
import time
import json
import os
import sys
import threading
from typing import Dict, List, Any, Optional, Callable
import logging
import psutil
import numpy as np
from concurrent.futures import ThreadPoolExecutor, as_completed
from collections import defaultdict
import random
import gc
import tracemalloc
from contextlib import contextmanager

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class OptimizedDSL:
    """ä¼˜åŒ–åçš„DSLå®ç°"""
    
    def __init__(self, seed: int = 7, workers: int = 8):
        self.seed = seed
        self.workers = workers
        self.cache = {}
        self.metrics = {"total_tasks": 0, "completed_tasks": 0, "failed_tasks": 0}
        self.history = []
        
        # ä¼˜åŒ–ï¼šä½¿ç”¨çº¿ç¨‹æ± è€Œä¸æ˜¯åˆ›å»ºæ–°çº¿ç¨‹
        self.executor = ThreadPoolExecutor(max_workers=workers)
        
        # ä¼˜åŒ–ï¼šä»»åŠ¡é˜Ÿåˆ—
        self.task_queue = []
        self.completed_tasks = []
        
        # ä¼˜åŒ–ï¼šæ‰¹å¤„ç†
        self.batch_size = 4
        self.batch_timeout = 0.1  # 100ms
        
    def add_task(self, name: str, prompt: str, agent: str, priority: int = 0):
        """æ·»åŠ ä»»åŠ¡åˆ°é˜Ÿåˆ—"""
        task = {
            "name": name,
            "prompt": prompt,
            "agent": agent,
            "priority": priority,
            "status": "pending",
            "result": None,
            "start_time": None,
            "end_time": None
        }
        self.task_queue.append(task)
        self.metrics["total_tasks"] += 1
        return task
    
    def _process_batch(self, tasks: List[Dict], llm_callable: Callable) -> List[Dict]:
        """æ‰¹é‡å¤„ç†ä»»åŠ¡"""
        results = []
        
        # ä¼˜åŒ–ï¼šå¹¶å‘å¤„ç†æ‰¹æ¬¡ä¸­çš„ä»»åŠ¡
        futures = []
        for task in tasks:
            future = self.executor.submit(self._execute_single_task, task, llm_callable)
            futures.append((task, future))
        
        # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
        for task, future in futures:
            try:
                result = future.result(timeout=30)
                task["result"] = result
                task["status"] = "completed"
                task["end_time"] = time.time()
                self.metrics["completed_tasks"] += 1
                results.append(task)
            except Exception as e:
                logger.error(f"ä»»åŠ¡ {task['name']} æ‰§è¡Œå¤±è´¥: {e}")
                task["status"] = "failed"
                task["end_time"] = time.time()
                self.metrics["failed_tasks"] += 1
                results.append(task)
        
        return results
    
    def _execute_single_task(self, task: Dict, llm_callable: Callable) -> str:
        """æ‰§è¡Œå•ä¸ªä»»åŠ¡"""
        task["start_time"] = time.time()
        
        # ä¼˜åŒ–ï¼šæ£€æŸ¥ç¼“å­˜
        cache_key = f"{task['prompt']}_{task['agent']}"
        if cache_key in self.cache:
            return self.cache[cache_key]
        
        # æ‰§è¡ŒLLMè°ƒç”¨
        result = llm_callable(task["prompt"], task["agent"])
        
        # ä¼˜åŒ–ï¼šç¼“å­˜ç»“æœ
        self.cache[cache_key] = result
        
        return result
    
    def run(self, llm_callable: Callable) -> Dict[str, Any]:
        """è¿è¡ŒDSL"""
        start_time = time.time()
        
        # ä¼˜åŒ–ï¼šæŒ‰ä¼˜å…ˆçº§æ’åºä»»åŠ¡
        self.task_queue.sort(key=lambda x: x["priority"], reverse=True)
        
        # ä¼˜åŒ–ï¼šæ‰¹é‡å¤„ç†ä»»åŠ¡
        batch_results = []
        for i in range(0, len(self.task_queue), self.batch_size):
            batch = self.task_queue[i:i + self.batch_size]
            batch_result = self._process_batch(batch, llm_callable)
            batch_results.extend(batch_result)
        
        end_time = time.time()
        execution_time = end_time - start_time
        
        # è®¡ç®—æ€§èƒ½æŒ‡æ ‡
        successful_tasks = len([t for t in batch_results if t["status"] == "completed"])
        throughput = successful_tasks / execution_time if execution_time > 0 else 0
        avg_latency = execution_time / successful_tasks if successful_tasks > 0 else 0
        
        return {
            "execution_time": execution_time,
            "throughput": throughput,
            "successful_tasks": successful_tasks,
            "total_tasks": len(self.task_queue),
            "avg_latency": avg_latency,
            "success_rate": successful_tasks / len(self.task_queue) if self.task_queue else 0,
            "results": batch_results
        }
    
    def cleanup(self):
        """æ¸…ç†èµ„æº"""
        self.executor.shutdown(wait=True)

class PerformanceOptimizer:
    """æ€§èƒ½ä¼˜åŒ–å™¨"""
    
    def __init__(self):
        self.api_key = os.environ.get('OPENAI_API_KEY')
        self.base_url = os.environ.get('OPENAI_API_BASE', 'https://api.openai.com/v1')
        
        if not self.api_key:
            logger.error("æœªè®¾ç½®OPENAI_API_KEY")
            sys.exit(1)
    
    def _create_llm_client(self):
        """åˆ›å»ºLLMå®¢æˆ·ç«¯"""
        try:
            from openai import OpenAI
            client = OpenAI(
                api_key=self.api_key,
                base_url=self.base_url,
                timeout=30,
                max_retries=2
            )
            return client
        except Exception as e:
            logger.error(f"åˆ›å»ºLLMå®¢æˆ·ç«¯å¤±è´¥: {e}")
            return None
    
    def _optimized_llm_call(self, prompt: str, role: str = None) -> str:
        """ä¼˜åŒ–çš„LLMè°ƒç”¨"""
        client = self._create_llm_client()
        if not client:
            raise Exception("æ— æ³•åˆ›å»ºLLMå®¢æˆ·ç«¯")
        
        try:
            completion = client.chat.completions.create(
                model="gpt-3.5-turbo",
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½åŸå¸‚ç®¡ç†åŠ©æ‰‹ï¼Œè´Ÿè´£å¤„ç†å„ç§åŸå¸‚è¿è¥ä»»åŠ¡ã€‚è¯·ç”¨ä¸­æ–‡ç®€æ´åœ°å›åº”ç”¨æˆ·çš„è¯·æ±‚ã€‚"},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.3,
                max_tokens=500
            )
            return completion.choices[0].message.content
        except Exception as e:
            logger.error(f"LLMè°ƒç”¨å¤±è´¥: {e}")
            raise e
    
    def test_optimized_dsl(self, scenario: str, agent_count: int) -> Dict[str, Any]:
        """æµ‹è¯•ä¼˜åŒ–åçš„DSL"""
        try:
            # åˆ›å»ºæµ‹è¯•ä»»åŠ¡
            test_tasks = {
                "simple_math": [
                    "è®¡ç®— 15 + 27",
                    "è®¡ç®— 100 - 45", 
                    "è®¡ç®— 8 * 7",
                    "è®¡ç®— 144 / 12"
                ],
                "text_processing": [
                    "å¤„ç†æ–‡æœ¬: æ™ºèƒ½åŸå¸‚ç®¡ç†",
                    "å¤„ç†æ–‡æœ¬: äº¤é€šä¼˜åŒ–ç³»ç»Ÿ",
                    "å¤„ç†æ–‡æœ¬: ç¯å¢ƒç›‘æµ‹æ•°æ®",
                    "å¤„ç†æ–‡æœ¬: å…¬å…±æœåŠ¡ç®¡ç†"
                ],
                "data_analysis": [
                    "åˆ†ææ•°æ®: äº¤é€šæµé‡æ¨¡å¼",
                    "åˆ†ææ•°æ®: èƒ½æºæ¶ˆè€—è¶‹åŠ¿",
                    "åˆ†ææ•°æ®: äººå£åˆ†å¸ƒç»Ÿè®¡",
                    "åˆ†ææ•°æ®: ç¯å¢ƒè´¨é‡æŒ‡æ ‡"
                ],
                "decision_logic": [
                    "å†³ç­–: äº¤é€šä¿¡å·ç¯è°ƒæ•´",
                    "å†³ç­–: åœè½¦ä½åˆ†é…ç­–ç•¥",
                    "å†³ç­–: ç´§æ€¥äº‹ä»¶å“åº”",
                    "å†³ç­–: èµ„æºåˆ†é…ä¼˜åŒ–"
                ]
            }
            
            tasks = test_tasks[scenario]
            
            # åˆ›å»ºä¼˜åŒ–åçš„DSL
            dsl = OptimizedDSL(seed=42, workers=min(agent_count, 8))
            
            # æ·»åŠ ä»»åŠ¡
            for i, task_prompt in enumerate(tasks[:agent_count]):
                dsl.add_task(f"task_{i}", task_prompt, f"agent_{i}", priority=i)
            
            # è¿è¡ŒDSL
            start_time = time.time()
            result = dsl.run(self._optimized_llm_call)
            end_time = time.time()
            
            # æ¸…ç†èµ„æº
            dsl.cleanup()
            
            return {
                "framework": "Our Optimized DSL",
                "scenario": scenario,
                "agent_count": agent_count,
                "execution_time": result["execution_time"],
                "throughput": result["throughput"],
                "success_rate": result["success_rate"],
                "successful_tasks": result["successful_tasks"],
                "total_tasks": result["total_tasks"],
                "avg_latency": result["avg_latency"],
                "status": "success",
                "memory_usage": 0,
                "api_type": "real_api"
            }
            
        except Exception as e:
            logger.error(f"ä¼˜åŒ–DSLæµ‹è¯•å¤±è´¥: {e}")
            return {
                "framework": "Our Optimized DSL",
                "scenario": scenario,
                "agent_count": agent_count,
                "execution_time": 0,
                "throughput": 0,
                "success_rate": 0,
                "successful_tasks": 0,
                "total_tasks": len(test_tasks[scenario][:agent_count]),
                "avg_latency": 0,
                "status": "error",
                "memory_usage": 0,
                "error": str(e),
                "api_type": "error"
            }
    
    def run_optimization_test(self) -> Dict[str, Any]:
        """è¿è¡Œä¼˜åŒ–æµ‹è¯•"""
        logger.info("ğŸš€ å¼€å§‹Our DSLæ€§èƒ½ä¼˜åŒ–æµ‹è¯•...")
        
        results = []
        test_scenarios = ["simple_math", "text_processing", "data_analysis", "decision_logic"]
        agent_counts = [1, 3, 5]
        
        total_tests = len(test_scenarios) * len(agent_counts)
        current_test = 0
        
        for scenario in test_scenarios:
            for agent_count in agent_counts:
                current_test += 1
                logger.info(f"ğŸ“Š æµ‹è¯•è¿›åº¦: {current_test}/{total_tests} - {scenario} - {agent_count} agents")
                
                result = self.test_optimized_dsl(scenario, agent_count)
                results.append(result)
                
                # å¼ºåˆ¶åƒåœ¾å›æ”¶
                gc.collect()
        
        # ä¿å­˜ç»“æœ
        output_file = "results/optimized_dsl_results.json"
        os.makedirs("results", exist_ok=True)
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump({"benchmark_results": results}, f, indent=2, ensure_ascii=False)
        
        logger.info(f"âœ… ä¼˜åŒ–æµ‹è¯•å®Œæˆï¼ç»“æœå·²ä¿å­˜åˆ° {output_file}")
        
        return {"benchmark_results": results}

def main():
    """ä¸»å‡½æ•°"""
    optimizer = PerformanceOptimizer()
    results = optimizer.run_optimization_test()
    
    # ç®€å•ç»Ÿè®¡
    print("\n" + "="*80)
    print("ğŸ¯ Our DSLæ€§èƒ½ä¼˜åŒ–æµ‹è¯•ç»“æœ")
    print("="*80)
    
    framework_stats = {}
    for result in results["benchmark_results"]:
        framework = result["framework"]
        if framework not in framework_stats:
            framework_stats[framework] = {
                "throughputs": [],
                "latencies": [],
                "success_rates": [],
                "execution_times": []
            }
        
        if result["status"] == "success":
            framework_stats[framework]["throughputs"].append(result["throughput"])
            framework_stats[framework]["latencies"].append(result["avg_latency"])
            framework_stats[framework]["success_rates"].append(result["success_rate"])
            framework_stats[framework]["execution_times"].append(result["execution_time"])
    
    print(f"\nğŸ“Š ä¼˜åŒ–åæ€§èƒ½:")
    print(f"{'æ¡†æ¶':<20} {'å¹³å‡ååé‡':<15} {'å¹³å‡å»¶è¿Ÿ':<15} {'æˆåŠŸç‡':<15}")
    print("-" * 70)
    
    for framework, stats in framework_stats.items():
        if stats["throughputs"]:
            avg_throughput = np.mean(stats["throughputs"])
            avg_latency = np.mean(stats["latencies"])
            avg_success_rate = np.mean(stats["success_rates"])
            avg_execution_time = np.mean(stats["execution_times"])
            
            print(f"{framework:<20} {avg_throughput:<15.2f} {avg_latency*1000:<15.3f} {avg_success_rate:<15.2%}")
    
    # ä¸åŸå§‹ç»“æœå¯¹æ¯”
    print(f"\nğŸ“ˆ æ€§èƒ½æå‡åˆ†æ:")
    print(f"   ä¼˜åŒ–é‡ç‚¹:")
    print(f"   â€¢ æ‰¹é‡å¤„ç†: å‡å°‘APIè°ƒç”¨å¼€é”€")
    print(f"   â€¢ å¹¶å‘æ‰§è¡Œ: æé«˜ä»»åŠ¡å¹¶è¡Œåº¦")
    print(f"   â€¢ ç¼“å­˜æœºåˆ¶: é¿å…é‡å¤è®¡ç®—")
    print(f"   â€¢ çº¿ç¨‹æ± : å‡å°‘çº¿ç¨‹åˆ›å»ºå¼€é”€")
    print(f"   â€¢ ä»»åŠ¡æ’åº: ä¼˜åŒ–ä»»åŠ¡æ‰§è¡Œé¡ºåº")

if __name__ == "__main__":
    main()
