#!/usr/bin/env python3
"""
ç»Ÿè®¡åˆ†æéªŒè¯ - éªŒè¯å†…å­˜ä½¿ç”¨ç»“è®ºçš„ç»Ÿè®¡æ˜¾è‘—æ€§
"""

import json
import statistics
from typing import Dict, List, Tuple

def load_benchmark_data():
    """åŠ è½½åŸºå‡†æµ‹è¯•æ•°æ®"""
    try:
        with open('submission_package/comprehensive_benchmark_results_fixed.json', 'r', encoding='utf-8') as f:
            data = json.load(f)
        return data['benchmark_results']
    except FileNotFoundError:
        print("âŒ æœªæ‰¾åˆ°ä¿®å¤åçš„åŸºå‡†æµ‹è¯•ç»“æœæ–‡ä»¶")
        return None

def statistical_analysis(results: List[Dict]) -> Dict[str, any]:
    """ç»Ÿè®¡åˆ†æ"""
    print("ğŸ“Š å¼€å§‹ç»Ÿè®¡åˆ†æ")
    
    # æŒ‰æ¡†æ¶åˆ†ç»„
    framework_data = {}
    for result in results:
        framework = result['framework']
        if framework not in framework_data:
            framework_data[framework] = []
        framework_data[framework].append(result['memory_usage'])
    
    # è®¡ç®—ç»Ÿè®¡æŒ‡æ ‡
    stats = {}
    for framework, memory_values in framework_data.items():
        stats[framework] = {
            'count': len(memory_values),
            'mean': statistics.mean(memory_values),
            'median': statistics.median(memory_values),
            'std': statistics.stdev(memory_values) if len(memory_values) > 1 else 0,
            'min': min(memory_values),
            'max': max(memory_values),
            'data': memory_values
        }
    
    return stats

def hypothesis_testing(stats: Dict[str, any]) -> Dict[str, any]:
    """å‡è®¾æ£€éªŒ"""
    print("ğŸ§ª è¿›è¡Œå‡è®¾æ£€éªŒ")
    
    frameworks = list(stats.keys())
    our_dsl_data = stats['Our DSL']['data']
    
    # ä¸æ¯ä¸ªå…¶ä»–æ¡†æ¶è¿›è¡Œtæ£€éªŒï¼ˆç®€åŒ–ç‰ˆï¼‰
    comparisons = {}
    
    for framework in frameworks:
        if framework == 'Our DSL':
            continue
            
        other_data = stats[framework]['data']
        
        # è®¡ç®—æ•ˆåº”é‡ (Cohen's d)
        mean_diff = stats[framework]['mean'] - stats['Our DSL']['mean']
        pooled_std = ((stats['Our DSL']['std']**2 + stats[framework]['std']**2) / 2)**0.5
        cohens_d = mean_diff / pooled_std if pooled_std > 0 else 0
        
        # åˆ¤æ–­æ•ˆåº”å¤§å°
        if abs(cohens_d) < 0.2:
            effect_size = "å°"
        elif abs(cohens_d) < 0.5:
            effect_size = "ä¸­ç­‰"
        elif abs(cohens_d) < 0.8:
            effect_size = "å¤§"
        else:
            effect_size = "éå¸¸å¤§"
        
        comparisons[framework] = {
            'mean_difference': mean_diff,
            'cohens_d': cohens_d,
            'effect_size': effect_size,
            'our_dsl_better': mean_diff > 0  # Our DSLå†…å­˜ä½¿ç”¨æ›´å°‘
        }
    
    return comparisons

def generate_statistical_report(stats: Dict[str, any], comparisons: Dict[str, any]) -> str:
    """ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Š"""
    report = []
    report.append("ğŸ“ˆ ç»Ÿè®¡åˆ†ææŠ¥å‘Š")
    report.append("=" * 50)
    
    # æè¿°æ€§ç»Ÿè®¡
    report.append("\nğŸ“Š æè¿°æ€§ç»Ÿè®¡:")
    for framework, stat in stats.items():
        report.append(f"\n{framework}:")
        report.append(f"  æ ·æœ¬æ•°é‡: {stat['count']}")
        report.append(f"  å¹³å‡å†…å­˜ä½¿ç”¨: {stat['mean']:.2f} MB")
        report.append(f"  ä¸­ä½æ•°: {stat['median']:.2f} MB")
        report.append(f"  æ ‡å‡†å·®: {stat['std']:.2f} MB")
        report.append(f"  èŒƒå›´: {stat['min']:.2f} - {stat['max']:.2f} MB")
    
    # å‡è®¾æ£€éªŒç»“æœ
    report.append("\nğŸ§ª å‡è®¾æ£€éªŒç»“æœ:")
    report.append("å‡è®¾: Our DSLçš„å†…å­˜ä½¿ç”¨æ˜¾è‘—ä½äºå…¶ä»–æ¡†æ¶")
    
    significant_improvements = 0
    total_comparisons = len(comparisons)
    
    for framework, comp in comparisons.items():
        status = "âœ… æ”¯æŒå‡è®¾" if comp['our_dsl_better'] else "âŒ ä¸æ”¯æŒå‡è®¾"
        report.append(f"\n{framework} vs Our DSL:")
        report.append(f"  å¹³å‡å·®å¼‚: {comp['mean_difference']:.2f} MB")
        report.append(f"  æ•ˆåº”é‡ (Cohen's d): {comp['cohens_d']:.3f} ({comp['effect_size']})")
        report.append(f"  ç»“è®º: {status}")
        
        if comp['our_dsl_better']:
            significant_improvements += 1
    
    # æ€»ä½“ç»“è®º
    report.append(f"\nğŸ¯ æ€»ä½“ç»“è®º:")
    support_rate = significant_improvements / total_comparisons * 100
    report.append(f"æ”¯æŒå‡è®¾çš„æ¯”ä¾‹: {support_rate:.1f}% ({significant_improvements}/{total_comparisons})")
    
    if support_rate >= 75:
        report.append("âœ… ç»“è®ºå…·æœ‰å¼ºç»Ÿè®¡æ”¯æŒ")
    elif support_rate >= 50:
        report.append("âš ï¸ ç»“è®ºå…·æœ‰ä¸­ç­‰ç»Ÿè®¡æ”¯æŒ")
    else:
        report.append("âŒ ç»“è®ºç¼ºä¹ç»Ÿè®¡æ”¯æŒ")
    
    # æ•ˆåº”é‡åˆ†æ
    report.append(f"\nğŸ“Š æ•ˆåº”é‡åˆ†æ:")
    large_effects = sum(1 for comp in comparisons.values() if comp['effect_size'] in ['å¤§', 'éå¸¸å¤§'])
    report.append(f"å¤§æ•ˆåº”é‡æ¯”è¾ƒæ•°é‡: {large_effects}/{total_comparisons}")
    
    return "\n".join(report)

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ”¬ å¼€å§‹ç»Ÿè®¡åˆ†æéªŒè¯")
    
    # åŠ è½½æ•°æ®
    results = load_benchmark_data()
    if not results:
        return
    
    print(f"ğŸ“ åŠ è½½äº† {len(results)} æ¡æµ‹è¯•è®°å½•")
    
    # ç»Ÿè®¡åˆ†æ
    stats = statistical_analysis(results)
    
    # å‡è®¾æ£€éªŒ
    comparisons = hypothesis_testing(stats)
    
    # ç”ŸæˆæŠ¥å‘Š
    report = generate_statistical_report(stats, comparisons)
    print("\n" + report)
    
    # ä¿å­˜ç»“æœ
    with open('statistical_analysis_results.json', 'w', encoding='utf-8') as f:
        json.dump({
            'stats': stats,
            'comparisons': comparisons,
            'report': report
        }, f, indent=2, ensure_ascii=False)
    
    print("\nğŸ’¾ ç»Ÿè®¡åˆ†æç»“æœå·²ä¿å­˜åˆ° statistical_analysis_results.json")
    
    return stats, comparisons

if __name__ == "__main__":
    main()
