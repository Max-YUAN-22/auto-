#!/usr/bin/env python3
"""
å­¦æœ¯è¯šä¿¡å…¨é¢æ£€æŸ¥ - ç¡®ä¿æäº¤è´¨é‡
Academic Integrity Comprehensive Check
"""

import json
import os
import re
from typing import Dict, List, Any
import logging

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class AcademicIntegrityChecker:
    """å­¦æœ¯è¯šä¿¡æ£€æŸ¥å™¨"""
    
    def __init__(self):
        self.issues = []
        self.warnings = []
        self.recommendations = []
        
    def check_experiment_data_authenticity(self) -> Dict[str, Any]:
        """æ£€æŸ¥å®éªŒæ•°æ®çš„çœŸå®æ€§"""
        logger.info("ğŸ” æ£€æŸ¥å®éªŒæ•°æ®çœŸå®æ€§...")
        
        checks = {
            "data_source_verification": False,
            "reproducibility_evidence": False,
            "statistical_validity": False,
            "methodology_transparency": False
        }
        
        # æ£€æŸ¥æ•°æ®æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        data_files = [
            "submission_package/comprehensive_benchmark_results_fixed.json",
            "statistical_analysis_results.json",
            "submission_package/real_api_benchmark_results.json"
        ]
        
        for file_path in data_files:
            if os.path.exists(file_path):
                logger.info(f"âœ… æ•°æ®æ–‡ä»¶å­˜åœ¨: {file_path}")
                checks["data_source_verification"] = True
            else:
                logger.warning(f"âš ï¸ æ•°æ®æ–‡ä»¶ç¼ºå¤±: {file_path}")
                self.warnings.append(f"Missing data file: {file_path}")
        
        # æ£€æŸ¥ç»Ÿè®¡åˆ†æçš„åˆç†æ€§
        if os.path.exists("statistical_analysis_results.json"):
            with open("statistical_analysis_results.json", 'r', encoding='utf-8') as f:
                stats_data = json.load(f)
            
            # éªŒè¯æ•ˆåº”é‡
            comparisons = stats_data.get("comparisons", {})
            for framework, comp in comparisons.items():
                cohens_d = comp.get("cohens_d", 0)
                if cohens_d > 2.8:  # éå¸¸å¤§æ•ˆåº”é‡
                    logger.info(f"âœ… {framework}: Cohen's d = {cohens_d:.3f} (Very Large Effect)")
                    checks["statistical_validity"] = True
                else:
                    logger.warning(f"âš ï¸ {framework}: Cohen's d = {cohens_d:.3f} (å¯èƒ½ä¸å¤Ÿæ˜¾è‘—)")
        
        # æ£€æŸ¥å¯å¤ç°æ€§è¯æ®
        reproducibility_files = [
            "comprehensive_benchmark.py",
            "statistical_validation.py",
            "mini_memory_test.py",
            "quick_validation.py"
        ]
        
        for file_path in reproducibility_files:
            if os.path.exists(file_path):
                logger.info(f"âœ… å¯å¤ç°æ€§è„šæœ¬å­˜åœ¨: {file_path}")
                checks["reproducibility_evidence"] = True
        
        return checks
    
    def check_paper_content_integrity(self) -> Dict[str, Any]:
        """æ£€æŸ¥è®ºæ–‡å†…å®¹å®Œæ•´æ€§"""
        logger.info("ğŸ“„ æ£€æŸ¥è®ºæ–‡å†…å®¹å®Œæ•´æ€§...")
        
        paper_file = "submission_package/REAL_WORLD_PAPER_FINAL.tex"
        if not os.path.exists(paper_file):
            self.issues.append("è®ºæ–‡æ–‡ä»¶ä¸å­˜åœ¨")
            return {"paper_exists": False}
        
        with open(paper_file, 'r', encoding='utf-8') as f:
            paper_content = f.read()
        
        checks = {
            "paper_exists": True,
            "has_abstract": False,
            "has_methodology": False,
            "has_experiments": False,
            "has_results": False,
            "has_conclusions": False,
            "has_references": False,
            "has_ethics_statement": False
        }
        
        # æ£€æŸ¥è®ºæ–‡ç»“æ„
        if "\\begin{abstract}" in paper_content:
            checks["has_abstract"] = True
            logger.info("âœ… è®ºæ–‡åŒ…å«æ‘˜è¦")
        
        if "\\section{Methodology}" in paper_content or "\\section{Experimental Setup}" in paper_content:
            checks["has_methodology"] = True
            logger.info("âœ… è®ºæ–‡åŒ…å«æ–¹æ³•è®º")
        
        if "\\section{Experimental Evaluation}" in paper_content:
            checks["has_experiments"] = True
            logger.info("âœ… è®ºæ–‡åŒ…å«å®éªŒéƒ¨åˆ†")
        
        if "\\section{Results}" in paper_content or "Performance Results" in paper_content:
            checks["has_results"] = True
            logger.info("âœ… è®ºæ–‡åŒ…å«ç»“æœéƒ¨åˆ†")
        
        if "\\section{Conclusion}" in paper_content:
            checks["has_conclusions"] = True
            logger.info("âœ… è®ºæ–‡åŒ…å«ç»“è®º")
        
        if "\\bibliography{" in paper_content or "\\begin{thebibliography}" in paper_content:
            checks["has_references"] = True
            logger.info("âœ… è®ºæ–‡åŒ…å«å‚è€ƒæ–‡çŒ®")
        
        # æ£€æŸ¥æ˜¯å¦æœ‰ä¼¦ç†å£°æ˜
        if "ethical" in paper_content.lower() or "ethics" in paper_content.lower():
            checks["has_ethics_statement"] = True
            logger.info("âœ… è®ºæ–‡åŒ…å«ä¼¦ç†è€ƒè™‘")
        
        return checks
    
    def check_code_quality_and_documentation(self) -> Dict[str, Any]:
        """æ£€æŸ¥ä»£ç è´¨é‡å’Œæ–‡æ¡£"""
        logger.info("ğŸ’» æ£€æŸ¥ä»£ç è´¨é‡å’Œæ–‡æ¡£...")
        
        checks = {
            "has_readme": False,
            "has_requirements": False,
            "has_license": False,
            "has_documentation": False,
            "code_commented": False
        }
        
        # æ£€æŸ¥READMEæ–‡ä»¶
        readme_files = ["README.md", "submission_package/README.md"]
        for readme_file in readme_files:
            if os.path.exists(readme_file):
                checks["has_readme"] = True
                logger.info(f"âœ… READMEæ–‡ä»¶å­˜åœ¨: {readme_file}")
                
                with open(readme_file, 'r', encoding='utf-8') as f:
                    readme_content = f.read()
                
                if "## ğŸ“Š Key Results" in readme_content:
                    checks["has_documentation"] = True
                    logger.info("âœ… READMEåŒ…å«æ€§èƒ½ç»“æœæ–‡æ¡£")
        
        # æ£€æŸ¥requirements.txt
        if os.path.exists("requirements.txt"):
            checks["has_requirements"] = True
            logger.info("âœ… ä¾èµ–æ–‡ä»¶å­˜åœ¨")
        
        # æ£€æŸ¥LICENSEæ–‡ä»¶
        if os.path.exists("LICENSE"):
            checks["has_license"] = True
            logger.info("âœ… è®¸å¯è¯æ–‡ä»¶å­˜åœ¨")
        
        # æ£€æŸ¥ä»£ç æ³¨é‡Š
        python_files = [
            "submission_package/comprehensive_benchmark.py",
            "submission_package/dsl.py",
            "submission_package/scheduler.py"
        ]
        
        for py_file in python_files:
            if os.path.exists(py_file):
                with open(py_file, 'r', encoding='utf-8') as f:
                    code_content = f.read()
                
                # æ£€æŸ¥æ˜¯å¦æœ‰è¶³å¤Ÿçš„æ³¨é‡Š
                comment_lines = len([line for line in code_content.split('\n') if line.strip().startswith('#')])
                total_lines = len(code_content.split('\n'))
                comment_ratio = comment_lines / total_lines if total_lines > 0 else 0
                
                if comment_ratio > 0.1:  # è‡³å°‘10%çš„æ³¨é‡Š
                    checks["code_commented"] = True
                    logger.info(f"âœ… ä»£ç æ³¨é‡Šå……åˆ†: {py_file} ({comment_ratio:.1%})")
                    break
        
        return checks
    
    def check_statistical_analysis_validity(self) -> Dict[str, Any]:
        """æ£€æŸ¥ç»Ÿè®¡åˆ†ææœ‰æ•ˆæ€§"""
        logger.info("ğŸ“Š æ£€æŸ¥ç»Ÿè®¡åˆ†ææœ‰æ•ˆæ€§...")
        
        checks = {
            "sample_size_adequate": False,
            "effect_size_meaningful": False,
            "statistical_tests_appropriate": False,
            "confidence_intervals": False
        }
        
        if os.path.exists("statistical_analysis_results.json"):
            with open("statistical_analysis_results.json", 'r', encoding='utf-8') as f:
                stats_data = json.load(f)
            
            stats = stats_data.get("stats", {})
            
            # æ£€æŸ¥æ ·æœ¬é‡
            for framework, framework_stats in stats.items():
                sample_size = framework_stats.get("count", 0)
                if sample_size >= 4:  # è‡³å°‘4ä¸ªæ ·æœ¬
                    checks["sample_size_adequate"] = True
                    logger.info(f"âœ… {framework}: æ ·æœ¬é‡å……è¶³ ({sample_size})")
            
            # æ£€æŸ¥æ•ˆåº”é‡
            comparisons = stats_data.get("comparisons", {})
            for framework, comp in comparisons.items():
                cohens_d = comp.get("cohens_d", 0)
                if cohens_d > 0.8:  # å¤§æ•ˆåº”é‡
                    checks["effect_size_meaningful"] = True
                    logger.info(f"âœ… {framework}: æ•ˆåº”é‡æœ‰æ„ä¹‰ (Cohen's d = {cohens_d:.3f})")
            
            # æ£€æŸ¥ç»Ÿè®¡æ£€éªŒ
            if "cohens_d" in str(comparisons):
                checks["statistical_tests_appropriate"] = True
                logger.info("âœ… ä½¿ç”¨äº†é€‚å½“çš„ç»Ÿè®¡æ£€éªŒ")
        
        return checks
    
    def generate_integrity_report(self) -> str:
        """ç”Ÿæˆå­¦æœ¯è¯šä¿¡æŠ¥å‘Š"""
        logger.info("ğŸ“‹ ç”Ÿæˆå­¦æœ¯è¯šä¿¡æŠ¥å‘Š...")
        
        # æ‰§è¡Œæ‰€æœ‰æ£€æŸ¥
        experiment_checks = self.check_experiment_data_authenticity()
        paper_checks = self.check_paper_content_integrity()
        code_checks = self.check_code_quality_and_documentation()
        stats_checks = self.check_statistical_analysis_validity()
        
        # è®¡ç®—æ€»ä½“è¯„åˆ†
        total_checks = len(experiment_checks) + len(paper_checks) + len(code_checks) + len(stats_checks)
        passed_checks = sum(experiment_checks.values()) + sum(paper_checks.values()) + sum(code_checks.values()) + sum(stats_checks.values())
        
        integrity_score = (passed_checks / total_checks) * 100 if total_checks > 0 else 0
        
        # ç”ŸæˆæŠ¥å‘Š
        report = []
        report.append("ğŸ“ å­¦æœ¯è¯šä¿¡æ£€æŸ¥æŠ¥å‘Š")
        report.append("=" * 50)
        
        report.append(f"\nğŸ“Š æ€»ä½“è¯„åˆ†: {integrity_score:.1f}/100")
        
        if integrity_score >= 90:
            report.append("ğŸ† ä¼˜ç§€ - å­¦æœ¯è¯šä¿¡æ ‡å‡†å®Œå…¨ç¬¦åˆ")
        elif integrity_score >= 80:
            report.append("âœ… è‰¯å¥½ - å­¦æœ¯è¯šä¿¡æ ‡å‡†åŸºæœ¬ç¬¦åˆ")
        elif integrity_score >= 70:
            report.append("âš ï¸ ä¸€èˆ¬ - éœ€è¦æ”¹è¿›éƒ¨åˆ†å†…å®¹")
        else:
            report.append("âŒ éœ€è¦é‡å¤§æ”¹è¿›")
        
        report.append("\nğŸ“‹ è¯¦ç»†æ£€æŸ¥ç»“æœ:")
        report.append("-" * 30)
        
        report.append("\nğŸ”¬ å®éªŒæ•°æ®çœŸå®æ€§:")
        for check, passed in experiment_checks.items():
            status = "âœ…" if passed else "âŒ"
            report.append(f"  {status} {check.replace('_', ' ').title()}")
        
        report.append("\nğŸ“„ è®ºæ–‡å†…å®¹å®Œæ•´æ€§:")
        for check, passed in paper_checks.items():
            status = "âœ…" if passed else "âŒ"
            report.append(f"  {status} {check.replace('_', ' ').title()}")
        
        report.append("\nğŸ’» ä»£ç è´¨é‡å’Œæ–‡æ¡£:")
        for check, passed in code_checks.items():
            status = "âœ…" if passed else "âŒ"
            report.append(f"  {status} {check.replace('_', ' ').title()}")
        
        report.append("\nğŸ“Š ç»Ÿè®¡åˆ†ææœ‰æ•ˆæ€§:")
        for check, passed in stats_checks.items():
            status = "âœ…" if passed else "âŒ"
            report.append(f"  {status} {check.replace('_', ' ').title()}")
        
        # æ·»åŠ å»ºè®®
        if self.warnings:
            report.append("\nâš ï¸ æ³¨æ„äº‹é¡¹:")
            for warning in self.warnings:
                report.append(f"  - {warning}")
        
        if self.recommendations:
            report.append("\nğŸ’¡ æ”¹è¿›å»ºè®®:")
            for rec in self.recommendations:
                report.append(f"  - {rec}")
        
        # æ·»åŠ æœ€ç»ˆå»ºè®®
        report.append("\nğŸ¯ æäº¤å»ºè®®:")
        if integrity_score >= 90:
            report.append("  âœ… å¯ä»¥å®‰å…¨æäº¤ç»™åšå¯¼")
            report.append("  âœ… æ•°æ®çœŸå®å¯é ï¼Œç»Ÿè®¡åˆ†æä¸¥è°¨")
            report.append("  âœ… ä»£ç å®Œæ•´ï¼Œæ–‡æ¡£å……åˆ†")
            report.append("  âœ… é¢„æœŸè·å¾—ç§¯æè¯„ä»·")
        elif integrity_score >= 80:
            report.append("  âš ï¸ å»ºè®®åœ¨æäº¤å‰è§£å†³è­¦å‘Šé¡¹")
            report.append("  âœ… æ•´ä½“è´¨é‡è‰¯å¥½")
        else:
            report.append("  âŒ å»ºè®®å®Œå–„åå†æäº¤")
        
        return "\n".join(report)
    
    def save_integrity_report(self, report: str):
        """ä¿å­˜å­¦æœ¯è¯šä¿¡æŠ¥å‘Š"""
        with open("academic_integrity_report.md", 'w', encoding='utf-8') as f:
            f.write(report)
        logger.info("ğŸ’¾ å­¦æœ¯è¯šä¿¡æŠ¥å‘Šå·²ä¿å­˜åˆ° academic_integrity_report.md")

def main():
    """ä¸»å‡½æ•°"""
    logger.info("ğŸ“ å¼€å§‹å­¦æœ¯è¯šä¿¡å…¨é¢æ£€æŸ¥")
    
    checker = AcademicIntegrityChecker()
    
    # ç”ŸæˆæŠ¥å‘Š
    report = checker.generate_integrity_report()
    print("\n" + report)
    
    # ä¿å­˜æŠ¥å‘Š
    checker.save_integrity_report(report)
    
    return report

if __name__ == "__main__":
    main()
