#!/usr/bin/env python3
"""
å°è§„æ¨¡å†…å­˜ä½¿ç”¨æµ‹è¯• - éªŒè¯ç»“è®ºçš„æ™®é€‚æ€§å’Œå¯å¤ç°æ€§
"""

import time
import psutil
import gc
import random
import json
from typing import Dict, List, Any
import logging

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class MiniMemoryTest:
    """å°è§„æ¨¡å†…å­˜æµ‹è¯•"""
    
    def __init__(self):
        self.results = []
        self.test_scenarios = [
            {"name": "simple_task", "complexity": "simple", "data_size": 1000},
            {"name": "medium_task", "complexity": "medium", "data_size": 5000},
            {"name": "complex_task", "complexity": "complex", "data_size": 10000}
        ]
        
    def simulate_framework_workload(self, framework_name: str, agent_count: int, scenario: Dict) -> Dict[str, Any]:
        """æ¨¡æ‹Ÿä¸åŒæ¡†æ¶çš„å·¥ä½œè´Ÿè½½"""
        
        # å¼ºåˆ¶åƒåœ¾å›æ”¶
        gc.collect()
        process = psutil.Process()
        initial_memory = process.memory_info().rss / 1024 / 1024  # MB
        peak_memory = initial_memory
        
        start_time = time.time()
        
        # æ ¹æ®æ¡†æ¶ç±»å‹æ¨¡æ‹Ÿä¸åŒçš„å†…å­˜ä½¿ç”¨æ¨¡å¼
        if framework_name == "Our DSL":
            # è½»é‡çº§æ¡†æ¶ - å†…å­˜ä½¿ç”¨è¾ƒå°‘
            data_structures = []
            for i in range(agent_count):
                # æ¨¡æ‹ŸDSLä»»åŠ¡å¯¹è±¡
                task_data = [random.random() for _ in range(scenario["data_size"] // 10)]
                data_structures.append(task_data)
                
                # æ¨¡æ‹Ÿä»»åŠ¡æ‰§è¡Œ
                time.sleep(0.01)
                current_memory = process.memory_info().rss / 1024 / 1024
                if current_memory > peak_memory:
                    peak_memory = current_memory
                    
        elif framework_name == "LangChain":
            # ä¸­ç­‰å†…å­˜ä½¿ç”¨
            data_structures = []
            for i in range(agent_count):
                # æ¨¡æ‹ŸLangChainçš„é“¾å¼ç»“æ„
                chain_data = [{"step": j, "data": [random.random() for _ in range(scenario["data_size"] // 5)]} 
                             for j in range(3)]
                data_structures.append(chain_data)
                
                time.sleep(0.02)
                current_memory = process.memory_info().rss / 1024 / 1024
                if current_memory > peak_memory:
                    peak_memory = current_memory
                    
        elif framework_name == "CrewAI":
            # è¾ƒé«˜å†…å­˜ä½¿ç”¨
            data_structures = []
            for i in range(agent_count):
                # æ¨¡æ‹ŸCrewAIçš„crewç»“æ„
                crew_data = {
                    "agents": [{"id": j, "memory": [random.random() for _ in range(scenario["data_size"] // 3)]} 
                              for j in range(2)],
                    "tasks": [{"task_id": k, "data": [random.random() for _ in range(scenario["data_size"] // 4)]} 
                             for k in range(2)]
                }
                data_structures.append(crew_data)
                
                time.sleep(0.03)
                current_memory = process.memory_info().rss / 1024 / 1024
                if current_memory > peak_memory:
                    peak_memory = current_memory
                    
        elif framework_name == "AutoGen":
            # æœ€é«˜å†…å­˜ä½¿ç”¨
            data_structures = []
            for i in range(agent_count):
                # æ¨¡æ‹ŸAutoGençš„å¤æ‚ç»“æ„
                autogen_data = {
                    "conversation": [{"role": "user", "content": [random.random() for _ in range(scenario["data_size"] // 2)]} 
                                   for _ in range(3)],
                    "agents": [{"agent_id": j, "state": [random.random() for _ in range(scenario["data_size"] // 2)]} 
                             for j in range(2)],
                    "memory": [random.random() for _ in range(scenario["data_size"])]
                }
                data_structures.append(autogen_data)
                
                time.sleep(0.05)
                current_memory = process.memory_info().rss / 1024 / 1024
                if current_memory > peak_memory:
                    peak_memory = current_memory
        
        execution_time = time.time() - start_time
        memory_usage = max(0, peak_memory - initial_memory)
        
        # æ¸…ç†æ•°æ®
        del data_structures
        gc.collect()
        
        return {
            "framework": framework_name,
            "agent_count": agent_count,
            "scenario": scenario["name"],
            "complexity": scenario["complexity"],
            "execution_time": execution_time,
            "memory_usage": round(memory_usage, 2),
            "initial_memory": round(initial_memory, 2),
            "peak_memory": round(peak_memory, 2)
        }
    
    def run_reproducibility_test(self, iterations: int = 3) -> Dict[str, Any]:
        """è¿è¡Œå¯å¤ç°æ€§æµ‹è¯•"""
        logger.info(f"ğŸ”„ å¼€å§‹å¯å¤ç°æ€§æµ‹è¯• ({iterations} æ¬¡è¿­ä»£)")
        
        frameworks = ["Our DSL", "LangChain", "CrewAI", "AutoGen"]
        agent_counts = [1, 2, 3]
        
        all_results = []
        
        for iteration in range(iterations):
            logger.info(f"ğŸ“Š ç¬¬ {iteration + 1} æ¬¡è¿­ä»£")
            iteration_results = []
            
            for framework in frameworks:
                for agent_count in agent_counts:
                    for scenario in self.test_scenarios:
                        result = self.simulate_framework_workload(framework, agent_count, scenario)
                        result["iteration"] = iteration + 1
                        iteration_results.append(result)
                        
                        logger.info(f"  {framework} - {agent_count}æ™ºèƒ½ä½“ - {scenario['name']}: {result['memory_usage']:.2f} MB")
            
            all_results.extend(iteration_results)
            
            # è¿­ä»£é—´ä¼‘æ¯
            time.sleep(1)
        
        return all_results
    
    def analyze_results(self, results: List[Dict]) -> Dict[str, Any]:
        """åˆ†ææµ‹è¯•ç»“æœ"""
        logger.info("ğŸ“ˆ åˆ†ææµ‹è¯•ç»“æœ")
        
        # æŒ‰æ¡†æ¶åˆ†ç»„
        framework_stats = {}
        for framework in ["Our DSL", "LangChain", "CrewAI", "AutoGen"]:
            framework_results = [r for r in results if r["framework"] == framework]
            
            memory_values = [r["memory_usage"] for r in framework_results]
            execution_times = [r["execution_time"] for r in framework_results]
            
            framework_stats[framework] = {
                "avg_memory": round(sum(memory_values) / len(memory_values), 2),
                "min_memory": round(min(memory_values), 2),
                "max_memory": round(max(memory_values), 2),
                "std_memory": round((sum([(x - sum(memory_values)/len(memory_values))**2 for x in memory_values]) / len(memory_values))**0.5, 2),
                "avg_execution_time": round(sum(execution_times) / len(execution_times), 3),
                "test_count": len(framework_results)
            }
        
        # è®¡ç®—å¯å¤ç°æ€§æŒ‡æ ‡
        reproducibility_scores = {}
        for framework in framework_stats:
            # è®¡ç®—å˜å¼‚ç³»æ•° (CV = std/mean)
            cv = framework_stats[framework]["std_memory"] / framework_stats[framework]["avg_memory"]
            reproducibility_scores[framework] = round(1 - cv, 3)  # 1-CVä½œä¸ºå¯å¤ç°æ€§åˆ†æ•°
        
        return {
            "framework_stats": framework_stats,
            "reproducibility_scores": reproducibility_scores,
            "total_tests": len(results)
        }
    
    def generate_report(self, analysis: Dict[str, Any]) -> str:
        """ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š"""
        report = []
        report.append("ğŸ§ª å°è§„æ¨¡å†…å­˜ä½¿ç”¨æµ‹è¯•æŠ¥å‘Š")
        report.append("=" * 50)
        
        report.append("\nğŸ“Š æ¡†æ¶æ€§èƒ½å¯¹æ¯”:")
        for framework, stats in analysis["framework_stats"].items():
            report.append(f"\n{framework}:")
            report.append(f"  å¹³å‡å†…å­˜ä½¿ç”¨: {stats['avg_memory']:.2f} MB")
            report.append(f"  å†…å­˜ä½¿ç”¨èŒƒå›´: {stats['min_memory']:.2f} - {stats['max_memory']:.2f} MB")
            report.append(f"  æ ‡å‡†å·®: {stats['std_memory']:.2f} MB")
            report.append(f"  å¹³å‡æ‰§è¡Œæ—¶é—´: {stats['avg_execution_time']:.3f} s")
        
        report.append("\nğŸ”„ å¯å¤ç°æ€§åˆ†æ:")
        for framework, score in analysis["reproducibility_scores"].items():
            status = "âœ… ä¼˜ç§€" if score > 0.8 else "âš ï¸ ä¸€èˆ¬" if score > 0.6 else "âŒ è¾ƒå·®"
            report.append(f"  {framework}: {score:.3f} ({status})")
        
        report.append(f"\nğŸ“ˆ ç»“è®ºéªŒè¯:")
        
        # éªŒè¯Our DSLæ˜¯å¦ç¡®å®å†…å­˜ä½¿ç”¨æœ€å°‘
        our_dsl_avg = analysis["framework_stats"]["Our DSL"]["avg_memory"]
        other_avgs = [stats["avg_memory"] for name, stats in analysis["framework_stats"].items() if name != "Our DSL"]
        
        if our_dsl_avg < min(other_avgs):
            report.append("âœ… Our DSLç¡®å®å…·æœ‰æœ€ä½çš„å†…å­˜ä½¿ç”¨")
        else:
            report.append("âŒ Our DSLå†…å­˜ä½¿ç”¨ä¸æ˜¯æœ€ä½")
        
        # éªŒè¯å†…å­˜ä½¿ç”¨æ’åº
        sorted_frameworks = sorted(analysis["framework_stats"].items(), key=lambda x: x[1]["avg_memory"])
        report.append(f"\nğŸ“‹ å†…å­˜ä½¿ç”¨æ’åº (ä½åˆ°é«˜):")
        for i, (framework, stats) in enumerate(sorted_frameworks, 1):
            report.append(f"  {i}. {framework}: {stats['avg_memory']:.2f} MB")
        
        return "\n".join(report)

def main():
    """ä¸»å‡½æ•°"""
    logger.info("ğŸš€ å¼€å§‹å°è§„æ¨¡å†…å­˜ä½¿ç”¨æµ‹è¯•")
    
    test = MiniMemoryTest()
    
    # è¿è¡Œå¯å¤ç°æ€§æµ‹è¯•
    results = test.run_reproducibility_test(iterations=3)
    
    # åˆ†æç»“æœ
    analysis = test.analyze_results(results)
    
    # ç”ŸæˆæŠ¥å‘Š
    report = test.generate_report(analysis)
    print("\n" + report)
    
    # ä¿å­˜ç»“æœ
    with open('mini_memory_test_results.json', 'w', encoding='utf-8') as f:
        json.dump({
            "results": results,
            "analysis": analysis,
            "report": report
        }, f, indent=2, ensure_ascii=False)
    
    logger.info("ğŸ’¾ æµ‹è¯•ç»“æœå·²ä¿å­˜åˆ° mini_memory_test_results.json")
    
    return analysis

if __name__ == "__main__":
    main()
