#!/usr/bin/env python3
"""
çœŸæ­£å…¬å¹³æµ‹è¯•ç»“æœåˆ†æ
Truly Fair Test Results Analysis
"""

import json
import numpy as np
from typing import Dict, List, Any

def load_results(file_path: str) -> Dict[str, Any]:
    """åŠ è½½æµ‹è¯•ç»“æœ"""
    with open(file_path, 'r', encoding='utf-8') as f:
        return json.load(f)

def analyze_truly_fair_results(results: Dict[str, Any]) -> Dict[str, Any]:
    """åˆ†æçœŸæ­£å…¬å¹³çš„æµ‹è¯•ç»“æœ"""
    framework_stats = {}
    
    for result in results["benchmark_results"]:
        framework = result["framework"]
        if framework not in framework_stats:
            framework_stats[framework] = {
                "throughputs": [],
                "latencies": [],
                "memory_usage": [],
                "success_rates": [],
                "execution_times": [],
                "successful_tasks": []
            }
        
        if result["status"] == "success":
            framework_stats[framework]["throughputs"].append(result["throughput"])
            framework_stats[framework]["latencies"].append(result["avg_latency"])
            framework_stats[framework]["memory_usage"].append(result["memory_usage"])
            framework_stats[framework]["success_rates"].append(result["success_rate"])
            framework_stats[framework]["execution_times"].append(result["execution_time"])
            framework_stats[framework]["successful_tasks"].append(result["successful_tasks"])
    
    # è®¡ç®—ç»Ÿè®¡æŒ‡æ ‡
    analysis = {}
    for framework, stats in framework_stats.items():
        if stats["throughputs"]:  # ç¡®ä¿æœ‰æ•°æ®
            analysis[framework] = {
                "avg_throughput": np.mean(stats["throughputs"]),
                "max_throughput": np.max(stats["throughputs"]),
                "min_throughput": np.min(stats["throughputs"]),
                "avg_latency": np.mean(stats["latencies"]),
                "avg_memory": np.mean(stats["memory_usage"]),
                "avg_success_rate": np.mean(stats["success_rates"]),
                "total_tests": len(stats["throughputs"]),
                "total_successful_tasks": sum(stats["successful_tasks"])
            }
    
    return analysis

def print_detailed_analysis(analysis: Dict[str, Any]):
    """æ‰“å°è¯¦ç»†åˆ†æ"""
    print("=" * 80)
    print("ğŸ¯ çœŸæ­£å…¬å¹³åŸºå‡†æµ‹è¯•ç»“æœåˆ†æ")
    print("=" * 80)
    
    print(f"\nğŸ“Š æ¡†æ¶æ€§èƒ½å¯¹æ¯”:")
    print(f"{'æ¡†æ¶':<15} {'å¹³å‡ååé‡':<15} {'å¹³å‡å»¶è¿Ÿ':<15} {'å¹³å‡å†…å­˜':<15} {'æˆåŠŸç‡':<15}")
    print("-" * 80)
    
    for framework, stats in analysis.items():
        print(f"{framework:<15} {stats['avg_throughput']:<15.2f} {stats['avg_latency']*1000:<15.3f} {stats['avg_memory']:<15.2f} {stats['avg_success_rate']:<15.2%}")
    
    print(f"\nğŸ† æ€§èƒ½æ’å:")
    
    # ååé‡æ’å
    throughput_ranking = sorted(analysis.items(), key=lambda x: x[1]["avg_throughput"], reverse=True)
    print(f"\n   ååé‡æ’å (tasks/sec):")
    for i, (name, stats) in enumerate(throughput_ranking, 1):
        print(f"     {i}. {name}: {stats['avg_throughput']:.2f}")
    
    # å»¶è¿Ÿæ’å
    latency_ranking = sorted(analysis.items(), key=lambda x: x[1]["avg_latency"])
    print(f"\n   å»¶è¿Ÿæ’å (ms):")
    for i, (name, stats) in enumerate(latency_ranking, 1):
        print(f"     {i}. {name}: {stats['avg_latency']*1000:.3f}")
    
    # å†…å­˜æ•ˆç‡æ’å
    memory_ranking = sorted(analysis.items(), key=lambda x: x[1]["avg_memory"])
    print(f"\n   å†…å­˜æ•ˆç‡æ’å (MB):")
    for i, (name, stats) in enumerate(memory_ranking, 1):
        print(f"     {i}. {name}: {stats['avg_memory']:.2f}")
    
    # æˆåŠŸç‡æ’å
    success_ranking = sorted(analysis.items(), key=lambda x: x[1]["avg_success_rate"], reverse=True)
    print(f"\n   æˆåŠŸç‡æ’å:")
    for i, (name, stats) in enumerate(success_ranking, 1):
        print(f"     {i}. {name}: {stats['avg_success_rate']:.2%}")
    
    # åˆ†æOur DSLçš„è¡¨ç°
    if "Our DSL" in analysis:
        our_dsl_stats = analysis["Our DSL"]
        print(f"\nğŸ” Our DSLè¯¦ç»†åˆ†æ:")
        print(f"   å¹³å‡ååé‡: {our_dsl_stats['avg_throughput']:.2f} tasks/sec")
        print(f"   å¹³å‡å»¶è¿Ÿ: {our_dsl_stats['avg_latency']*1000:.3f} ms")
        print(f"   å¹³å‡å†…å­˜ä½¿ç”¨: {our_dsl_stats['avg_memory']:.2f} MB")
        print(f"   æˆåŠŸç‡: {our_dsl_stats['avg_success_rate']:.2%}")
        print(f"   æ€»æµ‹è¯•æ•°: {our_dsl_stats['total_tests']}")
        print(f"   æˆåŠŸä»»åŠ¡æ•°: {our_dsl_stats['total_successful_tasks']}")
        
        # ä¸å…¶ä»–æ¡†æ¶å¯¹æ¯”
        other_frameworks = [name for name in analysis.keys() if name != "Our DSL"]
        if other_frameworks:
            best_throughput = max(analysis[f]["avg_throughput"] for f in other_frameworks)
            best_latency = min(analysis[f]["avg_latency"] for f in other_frameworks)
            best_memory = min(analysis[f]["avg_memory"] for f in other_frameworks)
            
            print(f"\nğŸ“ˆ ä¸æœ€ä½³æ€§èƒ½å¯¹æ¯”:")
            print(f"   ååé‡å·®è·: {our_dsl_stats['avg_throughput']/best_throughput:.2%}")
            print(f"   å»¶è¿Ÿå·®è·: {our_dsl_stats['avg_latency']/best_latency:.2%}")
            print(f"   å†…å­˜å·®è·: {our_dsl_stats['avg_memory']/best_memory:.2%}")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ“Š åŠ è½½çœŸæ­£å…¬å¹³åŸºå‡†æµ‹è¯•ç»“æœ...")
    
    try:
        results = load_results("results/truly_fair_benchmark_results.json")
        print("âœ… æˆåŠŸåŠ è½½çœŸæ­£å…¬å¹³åŸºå‡†æµ‹è¯•ç»“æœ")
    except FileNotFoundError:
        print("âŒ æœªæ‰¾åˆ°çœŸæ­£å…¬å¹³åŸºå‡†æµ‹è¯•ç»“æœæ–‡ä»¶")
        return
    
    print("\nğŸ” åˆ†ææµ‹è¯•ç»“æœ...")
    analysis = analyze_truly_fair_results(results)
    
    if not analysis:
        print("âŒ æ²¡æœ‰æœ‰æ•ˆçš„æµ‹è¯•ç»“æœ")
        return
    
    print_detailed_analysis(analysis)
    
    print("\n" + "=" * 80)
    print("âœ… åˆ†æå®Œæˆï¼")

if __name__ == "__main__":
    main()
